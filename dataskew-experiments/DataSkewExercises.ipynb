{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://towardsdatascience.com/six-spark-exercises-to-rule-them-all-242445b24565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row\n",
    "from numpy.random import rand\n",
    "from pyspark.sql.types import IntegerType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My machine has following configuration...\n",
    "- 6 cores with 12vCores\n",
    "- 32GB RAM\n",
    "\n",
    "Spark Standalone server:\n",
    "```\n",
    "cd /opt/softwares/spark-3.0.1-bin-hadoop3.2/\n",
    "\n",
    "export PYSPARK_PYTHON=/opt/envs/ai4e/bin/python\n",
    "export PYSPARK_DRIVER_PYTHON=/opt/envs/ai4e/bin/python\n",
    "\n",
    "sbin/start-all.sh\n",
    "sbin/stop-all.sh\n",
    "```\n",
    "Spark UI: [http://localhost:8080](http://localhost:8080)   \n",
    "Spark Master URL : spark://IMCHLT276:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://IMCHLT276:7077\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.cores.max\", \"6\") \\\n",
    "    .config(\"spark.local.dir\", \"/opt/tmp/spark-temp/\") \\\n",
    "    .appName(\"DataSkewness\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# https://stackoverflow.com/questions/27774443/is-it-safe-to-temporarily-rename-tmp-and-then-create-a-tmp-symlink-to-a-differe\n",
    "# sudo mount --bind /path/to/dir/with/plenty/of/space /tmp\n",
    "# sudo lsof /tmp # check for apps\n",
    "# sudo umount /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.142:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://IMCHLT276:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataSkewness</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f655e32c4d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Products  \n",
    "--------\n",
    ": product_id - The product ID  \n",
    ": product_name  - The product name   \n",
    ": price - The product price   \n",
    "\n",
    "Sellers  \n",
    "-------\n",
    ": seller_id  - The seller ID   \n",
    ": seller_name  - The seller name    \n",
    ": daily_target - The number of items (regardless of the product type) that the seller needs to hit his/her quota. For example, if the daily target is 100,000, the employee needs to sell 100,000 products he can hit the quota by selling 100,000 units of product_0, but also selling 30,000 units of product_1 and 70,000 units of product_2   \n",
    "\n",
    "Sales  \n",
    "-----\n",
    ": order_id  - The order ID   \n",
    ": product_id  - The single product sold in the order. All orders have exactly one product)   \n",
    ": seller_id  - The selling employee ID that sold the product   \n",
    ": date - The date of the order.    \n",
    ": num_pieces_sold - The number of units sold for the specific product in the order    \n",
    ": bill_raw_text  -  A string that represents the raw text of the bill associated with the order   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(path, is_print=False):\n",
    "    if is_print: print(f\"\\nReading {path}\")\n",
    "    df = spark.read.parquet(path)\n",
    "    if is_print: df.show()\n",
    "    if is_print: print(f\"Number of records in {path} is {df.count()}\" )\n",
    "    if is_print: print(\"_\"*80 + \"\\n\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataGenerator.ipynb\t products.csv\t   sales.csv\t  sellers_parquet\n",
      "DataSkewExercises.ipynb  products_parquet  sales_parquet\n",
      "data\t\t\t requirements.txt  sellers.csv\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df, sales_df, sellers_df = None, None, None\n",
    "def reload(is_print=False):\n",
    "    global products_df, sales_df, sellers_df\n",
    "    products_df = read_df(\"products_parquet\", is_print=is_print)\n",
    "    sellers_df = read_df(\"sellers_parquet\", is_print=is_print)\n",
    "    sales_df = read_df(\"sales_parquet/\", is_print=is_print)\n",
    "#     return products_df, sales_df, sellers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading products_parquet\n",
      "+----------+------------+-----+\n",
      "|product_id|product_name|price|\n",
      "+----------+------------+-----+\n",
      "|         0|   product_0|   22|\n",
      "|         1|   product_1|   30|\n",
      "|         2|   product_2|   91|\n",
      "|         3|   product_3|   37|\n",
      "|         4|   product_4|  145|\n",
      "|         5|   product_5|  128|\n",
      "|         6|   product_6|   66|\n",
      "|         7|   product_7|  145|\n",
      "|         8|   product_8|   51|\n",
      "|         9|   product_9|   44|\n",
      "|        10|  product_10|   53|\n",
      "|        11|  product_11|   13|\n",
      "|        12|  product_12|  104|\n",
      "|        13|  product_13|  102|\n",
      "|        14|  product_14|   24|\n",
      "|        15|  product_15|   14|\n",
      "|        16|  product_16|   38|\n",
      "|        17|  product_17|   72|\n",
      "|        18|  product_18|   16|\n",
      "|        19|  product_19|   46|\n",
      "+----------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of records in products_parquet is 75000000\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "Reading sellers_parquet\n",
      "+---------+-----------+------------+\n",
      "|seller_id|seller_name|daily_target|\n",
      "+---------+-----------+------------+\n",
      "|        0|   seller_0|     2500000|\n",
      "|        1|   seller_1|      257237|\n",
      "|        2|   seller_2|      754188|\n",
      "|        3|   seller_3|      310462|\n",
      "|        4|   seller_4|     1532808|\n",
      "|        5|   seller_5|     1199693|\n",
      "|        6|   seller_6|     1055915|\n",
      "|        7|   seller_7|     1946998|\n",
      "|        8|   seller_8|      547320|\n",
      "|        9|   seller_9|     1318051|\n",
      "+---------+-----------+------------+\n",
      "\n",
      "Number of records in sellers_parquet is 10\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "Reading sales_parquet/\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|       1|         0|        0|2020-07-10|             26|kyeibuumwlyhuwksx...|\n",
      "|       2|         0|        0|2020-07-08|             13|jfyuoyfkeyqkckwbu...|\n",
      "|       3|         0|        0|2020-07-05|             38|uyjihlzhzcswxcccx...|\n",
      "|       4|         0|        0|2020-07-05|             56|umnxvoqbdzpbwjqmz...|\n",
      "|       5|         0|        0|2020-07-05|             11|zmqexmaawmvdpqhih...|\n",
      "|       6|         0|        0|2020-07-01|             82|lmuhhkpyuoyslwmvX...|\n",
      "|       7|         0|        0|2020-07-04|             15|zoqweontumefxbgvu...|\n",
      "|       8|         0|        0|2020-07-08|             79|sgldfgtcxufasnvsc...|\n",
      "|       9|         0|        0|2020-07-10|             25|jnykelwjjebgkwgmu...|\n",
      "|      10|         0|        0|2020-07-08|              8|yywjfihneygcvfnyl...|\n",
      "|      11|         0|        0|2020-07-01|             10|nxwejyoeznltdhcam...|\n",
      "|      12|         0|        0|2020-07-06|             45|efmymeftivwsfljzt...|\n",
      "|      13|         0|        0|2020-07-10|             63|nxhvtospPhfnkavdy...|\n",
      "|      14|         0|        0|2020-07-03|             22|ypyusdsjzfpfbucnn...|\n",
      "|      15|         0|        0|2020-07-09|             75|ymjvbhaxffyjcwzyn...|\n",
      "|      16|         0|        0|2020-07-10|             83|phbcykkhvqsbkipwa...|\n",
      "|      17|         0|        0|2020-07-04|             54|qgnGqqnjmbqZytoug...|\n",
      "|      18|         0|        0|2020-07-04|             58|ozmllbabrnhebWcex...|\n",
      "|      19|         0|        0|2020-07-07|             33|kbrvXuzgiuinodtkg...|\n",
      "|      20|         0|        0|2020-07-09|             73|jnqjzaigjtqlfwpug...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of records in sales_parquet/ is 20000040\n",
      "________________________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.filter(F.col('order_id') == 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Find out how many orders, how many products and how many sellers are in the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"Sellers no: {sellers_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"Products no: {products_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"Sales no: {sales_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. How many products have been sold at least once?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Number of products sold at least once\")\n",
    "sales_df.agg(F.countDistinct(F.col(\"product_id\"))).show()\n",
    "# sales_df.agg({\"product_id\" : \"countDistinct\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sales_df.select(F.countDistinct(F.col(\"product_id\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Which is the product contained in more orders?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sales_df.groupBy(\"product_id\")\\\n",
    "    .agg(F.count(\"*\").alias(\"cnt\"))\\\n",
    "    .orderBy(F.col(\"cnt\").desc())\\\n",
    "    .limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. How many distinct products have been sold in each day?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      date|   cnt|\n",
      "+----------+------+\n",
      "|2020-07-06|100765|\n",
      "|2020-07-09|100501|\n",
      "|2020-07-01|100337|\n",
      "|2020-07-03|100017|\n",
      "|2020-07-02| 99807|\n",
      "|2020-07-05| 99796|\n",
      "|2020-07-04| 99791|\n",
      "|2020-07-07| 99756|\n",
      "|2020-07-08| 99662|\n",
      "|2020-07-10| 98973|\n",
      "+----------+------+\n",
      "\n",
      "CPU times: user 6.78 ms, sys: 0 ns, total: 6.78 ms\n",
      "Wall time: 8.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sales_df.groupBy(\"date\") \\\n",
    "        .agg(F.countDistinct(F.col(\"product_id\")).alias(\"cnt\"))\\\n",
    "        .orderBy(F.col(\"cnt\").desc()) \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.What is the average revenue of the orders?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 3 µs, total: 9 µs\n",
      "Wall time: 17.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the DAG visualization of following join..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [product_id#397, product_name#398, price#399, order_id#409, seller_id#411, date#412, num_pieces_sold#413, bill_raw_text#414]\n",
      "+- *(5) SortMergeJoin [product_id#397], [product_id#410], Inner\n",
      "   :- *(2) Sort [product_id#397 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(product_id#397, 200), true, [id=#1035]\n",
      "   :     +- *(1) Project [product_id#397, product_name#398, price#399]\n",
      "   :        +- *(1) Filter isnotnull(product_id#397)\n",
      "   :           +- *(1) ColumnarToRow\n",
      "   :              +- FileScan parquet [product_id#397,product_name#398,price#399] Batched: true, DataFilters: [isnotnull(product_id#397)], Format: Parquet, Location: InMemoryFileIndex[file:/opt/vlab/gyan42/pyspark-learning-ground/dataskew-experiments/products_par..., PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:string,product_name:string,price:string>\n",
      "   +- *(4) Sort [product_id#410 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(product_id#410, 200), true, [id=#1045]\n",
      "         +- *(3) Project [order_id#409, product_id#410, seller_id#411, date#412, num_pieces_sold#413, bill_raw_text#414]\n",
      "            +- *(3) Filter isnotnull(product_id#410)\n",
      "               +- *(3) ColumnarToRow\n",
      "                  +- FileScan parquet [order_id#409,product_id#410,seller_id#411,date#412,num_pieces_sold#413,bill_raw_text#414] Batched: true, DataFilters: [isnotnull(product_id#410)], Format: Parquet, Location: InMemoryFileIndex[file:/opt/vlab/gyan42/pyspark-learning-ground/dataskew-experiments/sales_parquet], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<order_id:string,product_id:string,seller_id:string,date:string,num_pieces_sold:string,bill...\n",
      "\n",
      "\n",
      "CPU times: user 3.75 ms, sys: 939 µs, total: 4.69 ms\n",
      "Wall time: 118 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = products_df.join(sales_df, on=\"product_id\", how=\"inner\")\n",
    "res.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000040"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "products_df = products_df.repartition(128)\n",
    "sales_df = sales_df.repartition(256)\n",
    "products_df.join(sales_df, on=\"product_id\").count() ### Will end up as timeout error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000040"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "products_df = products_df.withColumnRenamed(\"product_id\", \"repartition_id\").repartition(512, F.col(\"repartition_id\"))\n",
    "sales_df = sales_df.withColumnRenamed(\"product_id\", \"repartition_id\").repartition(512, F.col(\"repartition_id\"))     \n",
    "products_df.join(sales_df, on=\"repartition_id\").count() ### Will end up as timeout error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the average revenue without salting...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.53 µs\n",
      "== Physical Plan ==\n",
      "*(6) HashAggregate(keys=[], functions=[avg((cast(price#2 as double) * cast(num_pieces_sold#58 as double)))])\n",
      "+- Exchange SinglePartition, true, [id=#188]\n",
      "   +- *(5) HashAggregate(keys=[], functions=[partial_avg((cast(price#2 as double) * cast(num_pieces_sold#58 as double)))])\n",
      "      +- *(5) Project [price#2, num_pieces_sold#58]\n",
      "         +- *(5) SortMergeJoin [product_id#0], [product_id#55], Inner\n",
      "            :- *(2) Sort [product_id#0 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(product_id#0, 200), true, [id=#169]\n",
      "            :     +- *(1) Project [product_id#0, price#2]\n",
      "            :        +- *(1) Filter isnotnull(product_id#0)\n",
      "            :           +- *(1) ColumnarToRow\n",
      "            :              +- FileScan parquet [product_id#0,price#2] Batched: true, DataFilters: [isnotnull(product_id#0)], Format: Parquet, Location: InMemoryFileIndex[file:/opt/vlab/gyan42/pyspark-learning-ground/dataskew-experiments/products_par..., PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:string,price:string>\n",
      "            +- *(4) Sort [product_id#55 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(product_id#55, 200), true, [id=#179]\n",
      "                  +- *(3) Project [product_id#55, num_pieces_sold#58]\n",
      "                     +- *(3) Filter isnotnull(product_id#55)\n",
      "                        +- *(3) ColumnarToRow\n",
      "                           +- FileScan parquet [product_id#55,num_pieces_sold#58] Batched: true, DataFilters: [isnotnull(product_id#55)], Format: Parquet, Location: InMemoryFileIndex[file:/opt/vlab/gyan42/pyspark-learning-ground/dataskew-experiments/sales_parquet], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:string,num_pieces_sold:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "res = products_df.join(sales_df, on=\"product_id\", how=\"inner\").agg(F.avg(products_df[\"price\"] * sales_df[\"num_pieces_sold\"]))\n",
    "res.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.show() # Error : An error occurred while calling o76.showString.\n",
    "# : org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 14 (showString at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saltify method 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saltify(df, col_name, number_of_partition):\n",
    "    \"\"\"\n",
    "    Adds a new column names `col_name`_salted, which has concatenated values of `col_name` and number in the range of 0 to number_of_partition\n",
    "    new value = old_value-partition_id\n",
    "    \n",
    "    i.e in our use case, the product 0 is now split into 1024 partitions, the down size the product ids with less data will also try to get paritioned into 1024 partitions\n",
    "    Note: import pyspark.sql.functions as F\n",
    "    \"\"\"\n",
    "    salted_col = col_name + \"_salted\"  \n",
    "    return df.withColumn(\"dummy\", F.monotonically_increasing_id() % number_of_partition)\\\n",
    "            .withColumn(salted_col, F.concat(F.col(col_name), F.lit(\"-\"),F.col(\"dummy\")))\\\n",
    "            .drop(F.col(\"dummy\")).repartition(number_of_partition, F.col(salted_col)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = saltify(df=products_df, col_name=\"product_id\", number_of_partition=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- product_id_salted: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- num_pieces_sold: string (nullable = true)\n",
      " |-- bill_raw_text: string (nullable = true)\n",
      " |-- product_id_salted: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df = saltify(df=sales_df, col_name=\"product_id\", number_of_partition=1024)\n",
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_df.groupBy(\"product_id_salted\")\\\n",
    "#     .agg(F.count(\"*\").alias(\"cnt\"))\\\n",
    "#     .orderBy(F.col(\"cnt\").desc())\\\n",
    "#     .limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.48 µs\n",
      "+------------------------------+\n",
      "|avg((price * num_pieces_sold))|\n",
      "+------------------------------+\n",
      "|             1242.751587464154|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "products_df.join(sales_df, on=\"product_id_salted\", how=\"inner\").agg(F.avg(products_df[\"price\"] * sales_df[\"num_pieces_sold\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saltify Method 2: Salt only the skewed product id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Check and select the skewed keys \n",
    "# In this case we are retrieving the top 100 keys: these will be the only salted keys.\n",
    "results = sales_df.groupby(sales_df[\"product_id\"]).count().sort(F.col(\"count\").desc()).limit(100).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|product_id|replication|\n",
      "+----------+-----------+\n",
      "|         0|          0|\n",
      "|         0|          1|\n",
      "|         0|          2|\n",
      "|         0|          3|\n",
      "|         0|          4|\n",
      "|         0|          5|\n",
      "|         0|          6|\n",
      "|         0|          7|\n",
      "|         0|          8|\n",
      "|         0|          9|\n",
      "|         0|         10|\n",
      "|         0|         11|\n",
      "|         0|         12|\n",
      "|         0|         13|\n",
      "|         0|         14|\n",
      "|         0|         15|\n",
      "|         0|         16|\n",
      "|         0|         17|\n",
      "|         0|         18|\n",
      "|         0|         19|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2 - What we want to do is:\n",
    "#  a. Duplicate the entries that we have in the dimension table for the most common products, e.g.\n",
    "#       product_0 will become: product_0-1, product_0-2, product_0-3 and so on\n",
    "#  b. On the sales table, we are going to replace \"product_0\" with a random duplicate (e.g. some of them \n",
    "#     will be replaced with product_0-1, others with product_0-2, etc.)\n",
    "# Using the new \"salted\" key will unskew the join\n",
    "\n",
    "# Let's create a dataset to do the trick\n",
    "REPLICATION_FACTOR = 101\n",
    "l = []\n",
    "replicated_products = []\n",
    "for _r in results:\n",
    "    replicated_products.append(_r[\"product_id\"])\n",
    "    for _rep in range(0, REPLICATION_FACTOR):\n",
    "        l.append((_r[\"product_id\"], _rep))\n",
    "        \n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "replicated_df = rdd.map(lambda x: Row(product_id=x[0], replication=int(x[1])))\n",
    "replicated_df = spark.createDataFrame(replicated_df)\n",
    "replicated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Step 3: Generate the salted key\n",
    "products_df = products_df.join(F.broadcast(replicated_df), products_df[\"product_id\"] == replicated_df[\"product_id\"], \"left\"). \\\n",
    "    withColumn(\"salted_join_key\", F.when(replicated_df[\"replication\"].isNull(), products_df[\"product_id\"]).otherwise(\n",
    "    F.concat(replicated_df[\"product_id\"], F.lit(\"-\"), replicated_df[\"replication\"])))\n",
    "\n",
    "products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = sales_df.withColumn(\"salted_join_key\", F.when(sales_df[\"product_id\"].isin(replicated_products),\n",
    "                                                             F.concat(sales_df[\"product_id\"], F.lit(\"-\"),\n",
    "                                                                    F.lit(round(rand() * (REPLICATION_FACTOR - 1), 0)).cast(\n",
    "                                                                        IntegerType()))).otherwise(sales_df[\"product_id\"]))\n",
    "sales_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|avg((price * num_pieces_sold))|\n",
      "+------------------------------+\n",
      "|            1246.1338560822878|\n",
      "+------------------------------+\n",
      "\n",
      "None\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "#   Step 4: Finally let's do the join\n",
    "print(sales_df.join(products_df, sales_df[\"salted_join_key\"] == products_df[\"salted_join_key\"],\n",
    "                       \"inner\").\n",
    "      agg(F.avg(products_df[\"price\"] * sales_df[\"num_pieces_sold\"])).show())\n",
    "\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. For each seller, what is the average % contribution of an order to the seller's daily quota?**\n",
    "\n",
    "Example   \n",
    "If Seller_0 with `quota=250` has 3 orders:Order 1: 10 products sold    \n",
    "Order 2: 8 products sold    \n",
    "Order 3: 7 products soldThe average % contribution of orders to the seller's quota would be:    \n",
    "Order 1: 10/105 = 0.04   \n",
    "Order 2: 8/105 = 0.032     \n",
    "Order 3: 7/105 = 0.028Average % Contribution = (0.04+0.032+0.028)/3 = 0.03333    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+\n",
      "|seller_id|seller_name|daily_target|\n",
      "+---------+-----------+------------+\n",
      "|        0|   seller_0|     2500000|\n",
      "|        1|   seller_1|      257237|\n",
      "|        2|   seller_2|      754188|\n",
      "|        3|   seller_3|      310462|\n",
      "|        4|   seller_4|     1532808|\n",
      "|        5|   seller_5|     1199693|\n",
      "|        6|   seller_6|     1055915|\n",
      "|        7|   seller_7|     1946998|\n",
      "|        8|   seller_8|      547320|\n",
      "|        9|   seller_9|     1318051|\n",
      "+---------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sellers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|       1|         0|        0|2020-07-10|             26|kyeibuumwlyhuwksx...|\n",
      "|       2|         0|        0|2020-07-08|             13|jfyuoyfkeyqkckwbu...|\n",
      "|       3|         0|        0|2020-07-05|             38|uyjihlzhzcswxcccx...|\n",
      "|       4|         0|        0|2020-07-05|             56|umnxvoqbdzpbwjqmz...|\n",
      "|       5|         0|        0|2020-07-05|             11|zmqexmaawmvdpqhih...|\n",
      "|       6|         0|        0|2020-07-01|             82|lmuhhkpyuoyslwmvX...|\n",
      "|       7|         0|        0|2020-07-04|             15|zoqweontumefxbgvu...|\n",
      "|       8|         0|        0|2020-07-08|             79|sgldfgtcxufasnvsc...|\n",
      "|       9|         0|        0|2020-07-10|             25|jnykelwjjebgkwgmu...|\n",
      "|      10|         0|        0|2020-07-08|              8|yywjfihneygcvfnyl...|\n",
      "|      11|         0|        0|2020-07-01|             10|nxwejyoeznltdhcam...|\n",
      "|      12|         0|        0|2020-07-06|             45|efmymeftivwsfljzt...|\n",
      "|      13|         0|        0|2020-07-10|             63|nxhvtospPhfnkavdy...|\n",
      "|      14|         0|        0|2020-07-03|             22|ypyusdsjzfpfbucnn...|\n",
      "|      15|         0|        0|2020-07-09|             75|ymjvbhaxffyjcwzyn...|\n",
      "|      16|         0|        0|2020-07-10|             83|phbcykkhvqsbkipwa...|\n",
      "|      17|         0|        0|2020-07-04|             54|qgnGqqnjmbqZytoug...|\n",
      "|      18|         0|        0|2020-07-04|             58|ozmllbabrnhebWcex...|\n",
      "|      19|         0|        0|2020-07-07|             33|kbrvXuzgiuinodtkg...|\n",
      "|      20|         0|        0|2020-07-09|             73|jnqjzaigjtqlfwpug...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|seller_id|          avg(ratio)|\n",
      "+---------+--------------------+\n",
      "|        7|2.595228787788171E-5|\n",
      "|        3|1.628885370565939...|\n",
      "|        8| 9.21303037540886E-5|\n",
      "|        0|2.019885898946922...|\n",
      "|        5|4.211073965904021E-5|\n",
      "|        6|4.782147194369122E-5|\n",
      "|        9|3.837913136180238E-5|\n",
      "|        1|1.964233366461015E-4|\n",
      "|        4|3.296428039825816E-5|\n",
      "|        2|6.690408001060484E-5|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.join(F.broadcast(sellers_df), on=\"seller_id\", how=\"inner\")\\\n",
    ".withColumn(\"ratio\", sales_df[\"num_pieces_sold\"]/sellers_df[\"daily_target\"])\\\n",
    ".groupBy(\"seller_id\").agg(F.avg(\"ratio\")).alias(\"percent\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.Who are the second most selling and the least selling persons (sellers) for each product?**\n",
    "\n",
    "**Who are those for product with `product_id = 0`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **If a product has been sold by only one seller**, we’ll put it into a special category (category: Only seller or multiple sellers with the same quantity).\n",
    "- **If a product has been sold by more than one seller**, but all of them sold the same quantity, we are going to put them in the same category as if they were only a single seller for that product (category: Only seller or multiple sellers with the same quantity).\n",
    "- **If the “least selling” is also the “second selling”**, we will count it only as “second seller”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|       1|         0|        0|2020-07-10|             26|kyeibuumwlyhuwksx...|\n",
      "|       2|         0|        0|2020-07-08|             13|jfyuoyfkeyqkckwbu...|\n",
      "|       3|         0|        0|2020-07-05|             38|uyjihlzhzcswxcccx...|\n",
      "|       4|         0|        0|2020-07-05|             56|umnxvoqbdzpbwjqmz...|\n",
      "|       5|         0|        0|2020-07-05|             11|zmqexmaawmvdpqhih...|\n",
      "|       6|         0|        0|2020-07-01|             82|lmuhhkpyuoyslwmvX...|\n",
      "|       7|         0|        0|2020-07-04|             15|zoqweontumefxbgvu...|\n",
      "|       8|         0|        0|2020-07-08|             79|sgldfgtcxufasnvsc...|\n",
      "|       9|         0|        0|2020-07-10|             25|jnykelwjjebgkwgmu...|\n",
      "|      10|         0|        0|2020-07-08|              8|yywjfihneygcvfnyl...|\n",
      "|      11|         0|        0|2020-07-01|             10|nxwejyoeznltdhcam...|\n",
      "|      12|         0|        0|2020-07-06|             45|efmymeftivwsfljzt...|\n",
      "|      13|         0|        0|2020-07-10|             63|nxhvtospPhfnkavdy...|\n",
      "|      14|         0|        0|2020-07-03|             22|ypyusdsjzfpfbucnn...|\n",
      "|      15|         0|        0|2020-07-09|             75|ymjvbhaxffyjcwzyn...|\n",
      "|      16|         0|        0|2020-07-10|             83|phbcykkhvqsbkipwa...|\n",
      "|      17|         0|        0|2020-07-04|             54|qgnGqqnjmbqZytoug...|\n",
      "|      18|         0|        0|2020-07-04|             58|ozmllbabrnhebWcex...|\n",
      "|      19|         0|        0|2020-07-07|             33|kbrvXuzgiuinodtkg...|\n",
      "|      20|         0|        0|2020-07-09|             73|jnqjzaigjtqlfwpug...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the sum of sales for each product and seller pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[product_id#282, seller_id#283], functions=[sum(cast(num_pieces_sold#285 as double))])\n",
      "+- Exchange hashpartitioning(product_id#282, seller_id#283, 200), true, [id=#215]\n",
      "   +- *(1) HashAggregate(keys=[product_id#282, seller_id#283], functions=[partial_sum(cast(num_pieces_sold#285 as double))])\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [product_id#282,seller_id#283,num_pieces_sold#285] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/opt/vlab/gyan42/pyspark-learning-ground/dataskew-experiments/sales_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:string,seller_id:string,num_pieces_sold:string>\n",
      "\n",
      "\n",
      "+----------+---------+---------------+\n",
      "|product_id|seller_id|num_pieces_sold|\n",
      "+----------+---------+---------------+\n",
      "|  46681458|        9|           33.0|\n",
      "|  64958710|        5|           93.0|\n",
      "|  20420388|        2|           69.0|\n",
      "|   4584308|        4|           37.0|\n",
      "|  54457557|        1|           33.0|\n",
      "|  43280168|        9|           16.0|\n",
      "|  59282395|        5|           55.0|\n",
      "|  54378916|        3|           96.0|\n",
      "|  14638090|        4|           86.0|\n",
      "|  46033191|        2|           93.0|\n",
      "|  53136064|        8|            9.0|\n",
      "|  33987900|        2|           42.0|\n",
      "|  69515106|        1|           76.0|\n",
      "|  62796659|        5|           86.0|\n",
      "|  53239929|        4|           22.0|\n",
      "|  14487805|        5|           53.0|\n",
      "|  35896753|        1|           50.0|\n",
      "|  70161633|        8|            7.0|\n",
      "|   6713593|        2|           71.0|\n",
      "|   2138142|        3|           97.0|\n",
      "+----------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcuate the number of pieces sold by each seller for each product\n",
    "sales_df_grpd = sales_df.groupby(F.col(\"product_id\"), F.col(\"seller_id\")). \\\n",
    "    agg(F.sum(\"num_pieces_sold\").alias(\"num_pieces_sold\"))\n",
    "sales_df_grpd.explain()\n",
    "sales_df_grpd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add two new ranking columns: one that ranks the products’ sales in descending order and another one that ranks in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the window functions, one will sort ascending the other one descending. Partition by the product_id\n",
    "# and sort by the pieces sold\n",
    "window_desc = Window.partitionBy(F.col(\"product_id\")).orderBy(F.col(\"num_pieces_sold\").desc())\n",
    "window_asc = Window.partitionBy(F.col(\"product_id\")).orderBy(F.col(\"num_pieces_sold\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Window [dense_rank(num_pieces_sold#300) windowspecdefinition(product_id#282, num_pieces_sold#300 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_desc#334], [product_id#282], [num_pieces_sold#300 DESC NULLS LAST]\n",
      "+- *(4) Sort [product_id#282 ASC NULLS FIRST, num_pieces_sold#300 DESC NULLS LAST], false, 0\n",
      "   +- Window [dense_rank(num_pieces_sold#300) windowspecdefinition(product_id#282, num_pieces_sold#300 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_asc#321], [product_id#282], [num_pieces_sold#300 ASC NULLS FIRST]\n",
      "      +- *(3) Sort [product_id#282 ASC NULLS FIRST, num_pieces_sold#300 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(product_id#282, 200), true, [id=#293]\n",
      "            +- *(2) HashAggregate(keys=[product_id#282, seller_id#283], functions=[sum(cast(num_pieces_sold#285 as double))])\n",
      "               +- Exchange hashpartitioning(product_id#282, seller_id#283, 200), true, [id=#289]\n",
      "                  +- *(1) HashAggregate(keys=[product_id#282, seller_id#283], functions=[partial_sum(cast(num_pieces_sold#285 as double))])\n",
      "                     +- *(1) ColumnarToRow\n",
      "                        +- FileScan parquet [product_id#282,seller_id#283,num_pieces_sold#285] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/opt/vlab/gyan42/pyspark-learning-ground/dataskew-experiments/sales_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:string,seller_id:string,num_pieces_sold:string>\n",
      "\n",
      "\n",
      "+----------+---------+---------------+--------+---------+\n",
      "|product_id|seller_id|num_pieces_sold|rank_asc|rank_desc|\n",
      "+----------+---------+---------------+--------+---------+\n",
      "|  10005243|        6|           98.0|       1|        1|\n",
      "|  10023464|        9|           59.0|       1|        1|\n",
      "|  10050363|        6|           18.0|       1|        1|\n",
      "|  10089524|        2|           53.0|       1|        1|\n",
      "|  10122266|        2|           25.0|       1|        1|\n",
      "|  10134574|        3|           28.0|       1|        1|\n",
      "|  10150439|        4|           72.0|       1|        1|\n",
      "|  10158822|        9|           86.0|       1|        1|\n",
      "|  10160884|        7|           89.0|       1|        1|\n",
      "|  10172594|        2|           60.0|       1|        1|\n",
      "|  10175294|        7|           29.0|       1|        1|\n",
      "|   1017716|        9|           82.0|       1|        1|\n",
      "|  10200802|        2|           42.0|       1|        1|\n",
      "|  10215353|        1|           22.0|       1|        1|\n",
      "|  10218345|        4|           98.0|       1|        1|\n",
      "|  10220977|        5|           76.0|       1|        1|\n",
      "|  10255853|        7|           26.0|       1|        1|\n",
      "|  10288525|        6|           63.0|       1|        1|\n",
      "|  10304712|        9|           48.0|       1|        1|\n",
      "|  10324080|        6|            5.0|       1|        1|\n",
      "+----------+---------+---------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Dense Rank (to avoid holes)\n",
    "sales_df_grpd = sales_df_grpd.withColumn(\"rank_asc\", F.dense_rank().over(window_asc)). \\\n",
    "    withColumn(\"rank_desc\", F.dense_rank().over(window_desc))\n",
    "sales_df_grpd.explain()\n",
    "sales_df_grpd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset obtained in three pieces: one for each case that we want to handle (second top selling, least selling, single selling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [product_id#282 AS single_seller_product_id#367, seller_id#283 AS single_seller_seller_id#368, Only seller or multiple sellers with the same results AS type#369]\n",
      "+- *(5) Filter ((isnotnull(rank_asc#321) AND isnotnull(rank_desc#334)) AND (rank_asc#321 = rank_desc#334))\n",
      "   +- Window [dense_rank(num_pieces_sold#300) windowspecdefinition(product_id#282, num_pieces_sold#300 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_desc#334], [product_id#282], [num_pieces_sold#300 DESC NULLS LAST]\n",
      "      +- *(4) Sort [product_id#282 ASC NULLS FIRST, num_pieces_sold#300 DESC NULLS LAST], false, 0\n",
      "         +- Window [dense_rank(num_pieces_sold#300) windowspecdefinition(product_id#282, num_pieces_sold#300 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_asc#321], [product_id#282], [num_pieces_sold#300 ASC NULLS FIRST]\n",
      "            +- *(3) Sort [product_id#282 ASC NULLS FIRST, num_pieces_sold#300 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(product_id#282, 200), true, [id=#426]\n",
      "                  +- *(2) HashAggregate(keys=[product_id#282, seller_id#283], functions=[sum(cast(num_pieces_sold#285 as double))])\n",
      "                     +- Exchange hashpartitioning(product_id#282, seller_id#283, 200), true, [id=#422]\n",
      "                        +- *(1) HashAggregate(keys=[product_id#282, seller_id#283], functions=[partial_sum(cast(num_pieces_sold#285 as double))])\n",
      "                           +- *(1) ColumnarToRow\n",
      "                              +- FileScan parquet [product_id#282,seller_id#283,num_pieces_sold#285] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/opt/vlab/gyan42/pyspark-learning-ground/dataskew-experiments/sales_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:string,seller_id:string,num_pieces_sold:string>\n",
      "\n",
      "\n",
      "+------------------------+-----------------------+--------------------+\n",
      "|single_seller_product_id|single_seller_seller_id|                type|\n",
      "+------------------------+-----------------------+--------------------+\n",
      "|                10005243|                      6|Only seller or mu...|\n",
      "|                10023464|                      9|Only seller or mu...|\n",
      "|                10050363|                      6|Only seller or mu...|\n",
      "|                10089524|                      2|Only seller or mu...|\n",
      "|                10122266|                      2|Only seller or mu...|\n",
      "|                10134574|                      3|Only seller or mu...|\n",
      "|                10150439|                      4|Only seller or mu...|\n",
      "|                10158822|                      9|Only seller or mu...|\n",
      "|                10160884|                      7|Only seller or mu...|\n",
      "|                10172594|                      2|Only seller or mu...|\n",
      "|                10175294|                      7|Only seller or mu...|\n",
      "|                 1017716|                      9|Only seller or mu...|\n",
      "|                10200802|                      2|Only seller or mu...|\n",
      "|                10215353|                      1|Only seller or mu...|\n",
      "|                10218345|                      4|Only seller or mu...|\n",
      "|                10220977|                      5|Only seller or mu...|\n",
      "|                10255853|                      7|Only seller or mu...|\n",
      "|                10288525|                      6|Only seller or mu...|\n",
      "|                10304712|                      9|Only seller or mu...|\n",
      "|                10324080|                      6|Only seller or mu...|\n",
      "+------------------------+-----------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get products that only have one row OR the products in which multiple sellers sold the same amount\n",
    "# (i.e. all the employees that ever sold the product, sold the same exact amount)\n",
    "single_seller = sales_df_grpd.where(F.col(\"rank_asc\") == F.col(\"rank_desc\")).select(\n",
    "    F.col(\"product_id\").alias(\"single_seller_product_id\"), F.col(\"seller_id\").alias(\"single_seller_seller_id\"),\n",
    "    F.lit(\"Only seller or multiple sellers with the same results\").alias(\"type\"))\n",
    "single_seller.explain()\n",
    "single_seller.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) Project [product_id#282 AS second_seller_product_id#386, seller_id#283 AS second_seller_seller_id#387, Second top seller AS type#388]\n",
      "+- *(4) Filter (isnotnull(rank_desc#334) AND (rank_desc#334 = 2))\n",
      "   +- Window [dense_rank(num_pieces_sold#300) windowspecdefinition(product_id#282, num_pieces_sold#300 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_desc#334], [product_id#282], [num_pieces_sold#300 DESC NULLS LAST]\n",
      "      +- *(3) Sort [product_id#282 ASC NULLS FIRST, num_pieces_sold#300 DESC NULLS LAST], false, 0\n",
      "         +- Exchange hashpartitioning(product_id#282, 200), true, [id=#561]\n",
      "            +- *(2) HashAggregate(keys=[product_id#282, seller_id#283], functions=[sum(cast(num_pieces_sold#285 as double))])\n",
      "               +- Exchange hashpartitioning(product_id#282, seller_id#283, 200), true, [id=#557]\n",
      "                  +- *(1) HashAggregate(keys=[product_id#282, seller_id#283], functions=[partial_sum(cast(num_pieces_sold#285 as double))])\n",
      "                     +- *(1) ColumnarToRow\n",
      "                        +- FileScan parquet [product_id#282,seller_id#283,num_pieces_sold#285] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/opt/vlab/gyan42/pyspark-learning-ground/dataskew-experiments/sales_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:string,seller_id:string,num_pieces_sold:string>\n",
      "\n",
      "\n",
      "+------------------------+-----------------------+-----------------+\n",
      "|second_seller_product_id|second_seller_seller_id|             type|\n",
      "+------------------------+-----------------------+-----------------+\n",
      "|                12194170|                      3|Second top seller|\n",
      "|                14700981|                      1|Second top seller|\n",
      "|                16113262|                      7|Second top seller|\n",
      "|                17806195|                      5|Second top seller|\n",
      "|                19121475|                      3|Second top seller|\n",
      "|                19351340|                      6|Second top seller|\n",
      "|                22001601|                      4|Second top seller|\n",
      "|                22357251|                      3|Second top seller|\n",
      "|                24279067|                      3|Second top seller|\n",
      "|                24401456|                      6|Second top seller|\n",
      "|                27293252|                      6|Second top seller|\n",
      "|                29343804|                      4|Second top seller|\n",
      "|                  307787|                      1|Second top seller|\n",
      "|                 3086568|                      1|Second top seller|\n",
      "|                31598409|                      3|Second top seller|\n",
      "|                32858319|                      9|Second top seller|\n",
      "|                33915630|                      5|Second top seller|\n",
      "|                36993002|                      6|Second top seller|\n",
      "|                39748505|                      2|Second top seller|\n",
      "|                 3984141|                      9|Second top seller|\n",
      "+------------------------+-----------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the second top sellers\n",
    "second_seller = sales_df_grpd.where(F.col(\"rank_desc\") == 2).select(\n",
    "    F.col(\"product_id\").alias(\"second_seller_product_id\"), F.col(\"seller_id\").alias(\"second_seller_seller_id\"),\n",
    "    F.lit(\"Second top seller\").alias(\"type\")\n",
    ")\n",
    "second_seller.explain()\n",
    "second_seller.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calculating the “least selling”, we exclude those products that have a single seller and those where the least selling employee is also the second most selling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(17) Project [product_id#282, seller_id#283, Least Seller AS type#457]\n",
      "+- SortMergeJoin [seller_id#283, product_id#282], [second_seller_seller_id#387, second_seller_product_id#386], LeftAnti\n",
      "   :- SortMergeJoin [seller_id#283, product_id#282], [single_seller_seller_id#368, single_seller_product_id#367], LeftAnti\n",
      "   :  :- *(5) Sort [seller_id#283 ASC NULLS FIRST, product_id#282 ASC NULLS FIRST], false, 0\n",
      "   :  :  +- Exchange hashpartitioning(seller_id#283, product_id#282, 200), true, [id=#823]\n",
      "   :  :     +- *(4) Project [product_id#282, seller_id#283]\n",
      "   :  :        +- *(4) Filter (isnotnull(rank_asc#321) AND (rank_asc#321 = 1))\n",
      "   :  :           +- Window [dense_rank(num_pieces_sold#300) windowspecdefinition(product_id#282, num_pieces_sold#300 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_asc#321], [product_id#282], [num_pieces_sold#300 ASC NULLS FIRST]\n",
      "   :  :              +- *(3) Sort [product_id#282 ASC NULLS FIRST, num_pieces_sold#300 ASC NULLS FIRST], false, 0\n",
      "   :  :                 +- Exchange hashpartitioning(product_id#282, 200), true, [id=#814]\n",
      "   :  :                    +- *(2) HashAggregate(keys=[product_id#282, seller_id#283], functions=[sum(cast(num_pieces_sold#285 as double))])\n",
      "   :  :                       +- Exchange hashpartitioning(product_id#282, seller_id#283, 200), true, [id=#810]\n",
      "   :  :                          +- *(1) HashAggregate(keys=[product_id#282, seller_id#283], functions=[partial_sum(cast(num_pieces_sold#285 as double))])\n",
      "   :  :                             +- *(1) ColumnarToRow\n",
      "   :  :                                +- FileScan parquet [product_id#282,seller_id#283,num_pieces_sold#285] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/opt/vlab/gyan42/pyspark-learning-ground/dataskew-experiments/sales_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:string,seller_id:string,num_pieces_sold:string>\n",
      "   :  +- *(11) Sort [single_seller_seller_id#368 ASC NULLS FIRST, single_seller_product_id#367 ASC NULLS FIRST], false, 0\n",
      "   :     +- Exchange hashpartitioning(single_seller_seller_id#368, single_seller_product_id#367, 200), true, [id=#850]\n",
      "   :        +- *(10) Project [product_id#282 AS single_seller_product_id#367, seller_id#283 AS single_seller_seller_id#368]\n",
      "   :           +- *(10) Filter (((isnotnull(rank_asc#321) AND isnotnull(rank_desc#334)) AND (rank_asc#321 = rank_desc#334)) AND isnotnull(seller_id#283))\n",
      "   :              +- Window [dense_rank(num_pieces_sold#300) windowspecdefinition(product_id#282, num_pieces_sold#300 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_desc#334], [product_id#282], [num_pieces_sold#300 DESC NULLS LAST]\n",
      "   :                 +- *(9) Sort [product_id#282 ASC NULLS FIRST, num_pieces_sold#300 DESC NULLS LAST], false, 0\n",
      "   :                    +- Window [dense_rank(num_pieces_sold#300) windowspecdefinition(product_id#282, num_pieces_sold#300 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_asc#321], [product_id#282], [num_pieces_sold#300 ASC NULLS FIRST]\n",
      "   :                       +- *(8) Sort [product_id#282 ASC NULLS FIRST, num_pieces_sold#300 ASC NULLS FIRST], false, 0\n",
      "   :                          +- Exchange hashpartitioning(product_id#282, 200), true, [id=#837]\n",
      "   :                             +- *(7) HashAggregate(keys=[product_id#282, seller_id#283], functions=[sum(cast(num_pieces_sold#285 as double))])\n",
      "   :                                +- Exchange hashpartitioning(product_id#282, seller_id#283, 200), true, [id=#833]\n",
      "   :                                   +- *(6) HashAggregate(keys=[product_id#282, seller_id#283], functions=[partial_sum(cast(num_pieces_sold#285 as double))])\n",
      "   :                                      +- *(6) Project [product_id#282, seller_id#283, num_pieces_sold#285]\n",
      "   :                                         +- *(6) Filter isnotnull(product_id#282)\n",
      "   :                                            +- *(6) ColumnarToRow\n",
      "   :                                               +- FileScan parquet [product_id#282,seller_id#283,num_pieces_sold#285] Batched: true, DataFilters: [isnotnull(product_id#282)], Format: Parquet, Location: InMemoryFileIndex[file:/opt/vlab/gyan42/pyspark-learning-ground/dataskew-experiments/sales_parquet], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:string,seller_id:string,num_pieces_sold:string>\n",
      "   +- *(16) Sort [second_seller_seller_id#387 ASC NULLS FIRST, second_seller_product_id#386 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(second_seller_seller_id#387, second_seller_product_id#386, 200), true, [id=#946]\n",
      "         +- *(15) Project [product_id#282 AS second_seller_product_id#386, seller_id#283 AS second_seller_seller_id#387]\n",
      "            +- *(15) Filter ((isnotnull(rank_desc#334) AND (rank_desc#334 = 2)) AND isnotnull(seller_id#283))\n",
      "               +- Window [dense_rank(num_pieces_sold#300) windowspecdefinition(product_id#282, num_pieces_sold#300 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_desc#334], [product_id#282], [num_pieces_sold#300 DESC NULLS LAST]\n",
      "                  +- *(14) Sort [product_id#282 ASC NULLS FIRST, num_pieces_sold#300 DESC NULLS LAST], false, 0\n",
      "                     +- ReusedExchange [product_id#282, seller_id#283, num_pieces_sold#300], Exchange hashpartitioning(product_id#282, 200), true, [id=#837]\n",
      "\n",
      "\n",
      "+----------+---------+------------+\n",
      "|product_id|seller_id|        type|\n",
      "+----------+---------+------------+\n",
      "|  19986717|        1|Least Seller|\n",
      "|  40496308|        5|Least Seller|\n",
      "|  52606213|        7|Least Seller|\n",
      "|  14542470|        5|Least Seller|\n",
      "|  28592106|        5|Least Seller|\n",
      "|  17944574|        8|Least Seller|\n",
      "|  61475460|        7|Least Seller|\n",
      "|   3534470|        3|Least Seller|\n",
      "|  35669461|        4|Least Seller|\n",
      "|  32602520|        9|Least Seller|\n",
      "|  72017876|        1|Least Seller|\n",
      "|  67723231|        5|Least Seller|\n",
      "|  56011040|        5|Least Seller|\n",
      "|  34681047|        5|Least Seller|\n",
      "|  57735075|        9|Least Seller|\n",
      "|  18182299|        7|Least Seller|\n",
      "|  69790381|        5|Least Seller|\n",
      "|  31136332|        9|Least Seller|\n",
      "|  10978356|        7|Least Seller|\n",
      "|  20774718|        9|Least Seller|\n",
      "+----------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the least sellers and exclude those rows that are already included in the first piece\n",
    "# We also exclude the \"second top sellers\" that are also \"least sellers\"\n",
    "least_seller = sales_df_grpd.where(F.col(\"rank_asc\") == 1).select(\n",
    "    F.col(\"product_id\"), F.col(\"seller_id\"),\n",
    "    F.lit(\"Least Seller\").alias(\"type\")\n",
    ").join(single_seller, (sales_df_grpd[\"seller_id\"] == single_seller[\"single_seller_seller_id\"]) & (\n",
    "        sales_df_grpd[\"product_id\"] == single_seller[\"single_seller_product_id\"]), \"left_anti\"). \\\n",
    "    join(second_seller, (sales_df_grpd[\"seller_id\"] == second_seller[\"second_seller_seller_id\"]) & (\n",
    "        sales_df_grpd[\"product_id\"] == second_seller[\"second_seller_product_id\"]), \"left_anti\")\n",
    "\n",
    "least_seller.explain()\n",
    "least_seller.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the pieces back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Union\n",
      ":- *(17) Project [product_id#282, seller_id#283, Least Seller AS type#457]\n",
      ":  +- SortMergeJoin [seller_id#283, product_id#282], [second_seller_seller_id#387, second_seller_product_id#386], LeftAnti\n",
      ":     :- SortMergeJoin [seller_id#283, product_id#282], [single_seller_seller_id#368, single_seller_product_id#367], LeftAnti\n",
      ":     :  :- *(5) Sort [seller_id#283 ASC NULLS FIRST, product_id#282 ASC NULLS FIRST], false, 0\n",
      ":     :  :  +- Exchange hashpartitioning(seller_id#283, product_id#282, 200), true, [id=#1569]\n",
      ":     :  :     +- *(4) Project [product_id#282, seller_id#283]\n",
      ":     :  :        +- *(4) Filter (isnotnull(rank_asc#321) AND (rank_asc#321 = 1))\n",
      ":     :  :           +- Window [dense_rank(num_pieces_sold#300) windowspecdefinition(product_id#282, num_pieces_sold#300 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_asc#321], [product_id#282], [num_pieces_sold#300 ASC NULLS FIRST]\n",
      ":     :  :              +- *(3) Sort [product_id#282 ASC NULLS FIRST, num_pieces_sold#300 ASC NULLS FIRST], false, 0\n",
      ":     :  :                 +- Exchange hashpartitioning(product_id#282, 200), true, [id=#1560]\n",
      ":     :  :                    +- *(2) HashAggregate(keys=[product_id#282, seller_id#283], functions=[sum(cast(num_pieces_sold#285 as double))])\n",
      ":     :  :                       +- Exchange hashpartitioning(product_id#282, seller_id#283, 200), true, [id=#1556]\n",
      ":     :  :                          +- *(1) HashAggregate(keys=[product_id#282, seller_id#283], functions=[partial_sum(cast(num_pieces_sold#285 as double))])\n",
      ":     :  :                             +- *(1) ColumnarToRow\n",
      ":     :  :                                +- FileScan parquet [product_id#282,seller_id#283,num_pieces_sold#285] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/opt/vlab/gyan42/pyspark-learning-ground/dataskew-experiments/sales_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:string,seller_id:string,num_pieces_sold:string>\n",
      ":     :  +- *(11) Sort [single_seller_seller_id#368 ASC NULLS FIRST, single_seller_product_id#367 ASC NULLS FIRST], false, 0\n",
      ":     :     +- Exchange hashpartitioning(single_seller_seller_id#368, single_seller_product_id#367, 200), true, [id=#1596]\n",
      ":     :        +- *(10) Project [product_id#282 AS single_seller_product_id#367, seller_id#283 AS single_seller_seller_id#368]\n",
      ":     :           +- *(10) Filter (((isnotnull(rank_asc#321) AND isnotnull(rank_desc#334)) AND (rank_asc#321 = rank_desc#334)) AND isnotnull(seller_id#283))\n",
      ":     :              +- Window [dense_rank(num_pieces_sold#300) windowspecdefinition(product_id#282, num_pieces_sold#300 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_desc#334], [product_id#282], [num_pieces_sold#300 DESC NULLS LAST]\n",
      ":     :                 +- *(9) Sort [product_id#282 ASC NULLS FIRST, num_pieces_sold#300 DESC NULLS LAST], false, 0\n",
      ":     :                    +- Window [dense_rank(num_pieces_sold#300) windowspecdefinition(product_id#282, num_pieces_sold#300 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_asc#321], [product_id#282], [num_pieces_sold#300 ASC NULLS FIRST]\n",
      ":     :                       +- *(8) Sort [product_id#282 ASC NULLS FIRST, num_pieces_sold#300 ASC NULLS FIRST], false, 0\n",
      ":     :                          +- Exchange hashpartitioning(product_id#282, 200), true, [id=#1583]\n",
      ":     :                             +- *(7) HashAggregate(keys=[product_id#282, seller_id#283], functions=[sum(cast(num_pieces_sold#285 as double))])\n",
      ":     :                                +- Exchange hashpartitioning(product_id#282, seller_id#283, 200), true, [id=#1579]\n",
      ":     :                                   +- *(6) HashAggregate(keys=[product_id#282, seller_id#283], functions=[partial_sum(cast(num_pieces_sold#285 as double))])\n",
      ":     :                                      +- *(6) Project [product_id#282, seller_id#283, num_pieces_sold#285]\n",
      ":     :                                         +- *(6) Filter isnotnull(product_id#282)\n",
      ":     :                                            +- *(6) ColumnarToRow\n",
      ":     :                                               +- FileScan parquet [product_id#282,seller_id#283,num_pieces_sold#285] Batched: true, DataFilters: [isnotnull(product_id#282)], Format: Parquet, Location: InMemoryFileIndex[file:/opt/vlab/gyan42/pyspark-learning-ground/dataskew-experiments/sales_parquet], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:string,seller_id:string,num_pieces_sold:string>\n",
      ":     +- *(16) Sort [second_seller_seller_id#387 ASC NULLS FIRST, second_seller_product_id#386 ASC NULLS FIRST], false, 0\n",
      ":        +- Exchange hashpartitioning(second_seller_seller_id#387, second_seller_product_id#386, 200), true, [id=#1731]\n",
      ":           +- *(15) Project [product_id#282 AS second_seller_product_id#386, seller_id#283 AS second_seller_seller_id#387]\n",
      ":              +- *(15) Filter ((isnotnull(rank_desc#334) AND (rank_desc#334 = 2)) AND isnotnull(seller_id#283))\n",
      ":                 +- Window [dense_rank(num_pieces_sold#300) windowspecdefinition(product_id#282, num_pieces_sold#300 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_desc#334], [product_id#282], [num_pieces_sold#300 DESC NULLS LAST]\n",
      ":                    +- *(14) Sort [product_id#282 ASC NULLS FIRST, num_pieces_sold#300 DESC NULLS LAST], false, 0\n",
      ":                       +- ReusedExchange [product_id#282, seller_id#283, num_pieces_sold#300], Exchange hashpartitioning(product_id#282, 200), true, [id=#1583]\n",
      ":- *(21) Project [product_id#282, seller_id#283, Second top seller AS type#388]\n",
      ":  +- *(21) Filter (isnotnull(rank_desc#334) AND (rank_desc#334 = 2))\n",
      ":     +- Window [dense_rank(num_pieces_sold#300) windowspecdefinition(product_id#282, num_pieces_sold#300 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_desc#334], [product_id#282], [num_pieces_sold#300 DESC NULLS LAST]\n",
      ":        +- *(20) Sort [product_id#282 ASC NULLS FIRST, num_pieces_sold#300 DESC NULLS LAST], false, 0\n",
      ":           +- ReusedExchange [product_id#282, seller_id#283, num_pieces_sold#300], Exchange hashpartitioning(product_id#282, 200), true, [id=#1560]\n",
      "+- *(26) Project [product_id#282, seller_id#283, Only seller or multiple sellers with the same results AS type#369]\n",
      "   +- *(26) Filter ((isnotnull(rank_asc#321) AND isnotnull(rank_desc#334)) AND (rank_asc#321 = rank_desc#334))\n",
      "      +- Window [dense_rank(num_pieces_sold#300) windowspecdefinition(product_id#282, num_pieces_sold#300 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_desc#334], [product_id#282], [num_pieces_sold#300 DESC NULLS LAST]\n",
      "         +- *(25) Sort [product_id#282 ASC NULLS FIRST, num_pieces_sold#300 DESC NULLS LAST], false, 0\n",
      "            +- Window [dense_rank(num_pieces_sold#300) windowspecdefinition(product_id#282, num_pieces_sold#300 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_asc#321], [product_id#282], [num_pieces_sold#300 ASC NULLS FIRST]\n",
      "               +- *(24) Sort [product_id#282 ASC NULLS FIRST, num_pieces_sold#300 ASC NULLS FIRST], false, 0\n",
      "                  +- ReusedExchange [product_id#282, seller_id#283, num_pieces_sold#300], Exchange hashpartitioning(product_id#282, 200), true, [id=#1560]\n",
      "\n",
      "\n",
      "+----------+---------+------------+\n",
      "|product_id|seller_id|        type|\n",
      "+----------+---------+------------+\n",
      "|  19986717|        1|Least Seller|\n",
      "|  40496308|        5|Least Seller|\n",
      "|  52606213|        7|Least Seller|\n",
      "|  14542470|        5|Least Seller|\n",
      "|  28592106|        5|Least Seller|\n",
      "|  17944574|        8|Least Seller|\n",
      "|  61475460|        7|Least Seller|\n",
      "|   3534470|        3|Least Seller|\n",
      "|  35669461|        4|Least Seller|\n",
      "|  32602520|        9|Least Seller|\n",
      "|  72017876|        1|Least Seller|\n",
      "|  67723231|        5|Least Seller|\n",
      "|  56011040|        5|Least Seller|\n",
      "|  34681047|        5|Least Seller|\n",
      "|  57735075|        9|Least Seller|\n",
      "|  18182299|        7|Least Seller|\n",
      "|  69790381|        5|Least Seller|\n",
      "|  31136332|        9|Least Seller|\n",
      "|  10978356|        7|Least Seller|\n",
      "|  20774718|        9|Least Seller|\n",
      "+----------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Union all the pieces\n",
    "union_table = least_seller.select(\n",
    "    F.col(\"product_id\"),\n",
    "    F.col(\"seller_id\"),\n",
    "    F.col(\"type\")\n",
    ").union(second_seller.select(\n",
    "    F.col(\"second_seller_product_id\").alias(\"product_id\"),\n",
    "    F.col(\"second_seller_seller_id\").alias(\"seller_id\"),\n",
    "    F.col(\"type\")\n",
    ")).union(single_seller.select(\n",
    "    F.col(\"single_seller_product_id\").alias(\"product_id\"),\n",
    "    F.col(\"single_seller_seller_id\").alias(\"seller_id\"),\n",
    "    F.col(\"type\")\n",
    "))\n",
    "union_table.explain()\n",
    "union_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'col' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-53b1dae17e4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Which are the second top seller and least seller of product 0?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0munion_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"product_id\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'col' is not defined"
     ]
    }
   ],
   "source": [
    "# Which are the second top seller and least seller of product 0?\n",
    "union_table.where(F.col(\"product_id\") == 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.**\n",
    "Create a new column called \"hashed_bill\" defined as follows:\n",
    "\n",
    "- if the order_id is even: apply MD5 hashing iteratively to the bill_raw_text field, once for each 'A' (capital 'A') present in the text. E.g. if the bill text is 'nbAAnllA', you would apply hashing three times iteratively (only if the order number is even)\n",
    "\n",
    "- if the order_id is odd: apply SHA256 hashing to the bill textFinally, check if there are any duplicate on the new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|       1|         0|        0|2020-07-10|             26|kyeibuumwlyhuwksx...|\n",
      "|       2|         0|        0|2020-07-08|             13|jfyuoyfkeyqkckwbu...|\n",
      "|       3|         0|        0|2020-07-05|             38|uyjihlzhzcswxcccx...|\n",
      "|       4|         0|        0|2020-07-05|             56|umnxvoqbdzpbwjqmz...|\n",
      "|       5|         0|        0|2020-07-05|             11|zmqexmaawmvdpqhih...|\n",
      "|       6|         0|        0|2020-07-01|             82|lmuhhkpyuoyslwmvX...|\n",
      "|       7|         0|        0|2020-07-04|             15|zoqweontumefxbgvu...|\n",
      "|       8|         0|        0|2020-07-08|             79|sgldfgtcxufasnvsc...|\n",
      "|       9|         0|        0|2020-07-10|             25|jnykelwjjebgkwgmu...|\n",
      "|      10|         0|        0|2020-07-08|              8|yywjfihneygcvfnyl...|\n",
      "|      11|         0|        0|2020-07-01|             10|nxwejyoeznltdhcam...|\n",
      "|      12|         0|        0|2020-07-06|             45|efmymeftivwsfljzt...|\n",
      "|      13|         0|        0|2020-07-10|             63|nxhvtospPhfnkavdy...|\n",
      "|      14|         0|        0|2020-07-03|             22|ypyusdsjzfpfbucnn...|\n",
      "|      15|         0|        0|2020-07-09|             75|ymjvbhaxffyjcwzyn...|\n",
      "|      16|         0|        0|2020-07-10|             83|phbcykkhvqsbkipwa...|\n",
      "|      17|         0|        0|2020-07-04|             54|qgnGqqnjmbqZytoug...|\n",
      "|      18|         0|        0|2020-07-04|             58|ozmllbabrnhebWcex...|\n",
      "|      19|         0|        0|2020-07-07|             33|kbrvXuzgiuinodtkg...|\n",
      "|      20|         0|        0|2020-07-09|             73|jnqjzaigjtqlfwpug...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- num_pieces_sold: string (nullable = true)\n",
      " |-- bill_raw_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload()\n",
    "sales_df.show()\n",
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib \n",
    "def get_hash(order_id, text):\n",
    "    res = text\n",
    "    if int(order_id) % 2 == 0:\n",
    "        res = hashlib.md5(bytes(res, 'utf-8')).hexdigest()\n",
    "        for c in text:\n",
    "            if c == 'A':\n",
    "                res = hashlib.md5(bytes(res, 'utf-8')).hexdigest()\n",
    "    else:\n",
    "        res = hashlib.sha256(bytes(res, 'utf-8')).hexdigest()\n",
    "    return res\n",
    "        \n",
    "get_hash_udf = F.udf(get_hash, StringType())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'67d25a783609ce62d5456dc297c05dfd'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_hash(\"24\", \"jfyuoyfkAAeyqkckwbu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Project [order_id#114, product_id#115, seller_id#116, date#117, num_pieces_sold#118, bill_raw_text#119, pythonUDF0#160 AS hashed_bill#152]\n",
      "+- BatchEvalPython [get_hash(order_id#114, bill_raw_text#119)], [pythonUDF0#160]\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [order_id#114,product_id#115,seller_id#116,date#117,num_pieces_sold#118,bill_raw_text#119] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/opt/vlab/gyan42/pyspark-learning-ground/dataskew-experiments/sales_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,product_id:string,seller_id:string,date:string,num_pieces_sold:string,bill...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sales_df.withColumn(\"hashed_bill\", get_hash_udf(\"order_id\", \"bill_raw_text\")).show()\n",
    "hashed_df = sales_df.withColumn(\"hashed_bill\", get_hash_udf(F.col(\"order_id\"), F.col(\"bill_raw_text\")))\n",
    "hashed_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+---------------+--------------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|         hashed_bill|\n",
      "+--------+----------+---------+----------+---------------+--------------------+--------------------+\n",
      "|       1|         0|        0|2020-07-10|             26|kyeibuumwlyhuwksx...|f6fa2a8be04a4ead6...|\n",
      "|       2|         0|        0|2020-07-08|             13|jfyuoyfkeyqkckwbu...|jfyuoyfkeyqkckwbu...|\n",
      "|       3|         0|        0|2020-07-05|             38|uyjihlzhzcswxcccx...|416376a64cd652e7b...|\n",
      "|       4|         0|        0|2020-07-05|             56|umnxvoqbdzpbwjqmz...|umnxvoqbdzpbwjqmz...|\n",
      "|       5|         0|        0|2020-07-05|             11|zmqexmaawmvdpqhih...|787d361b162a6aa1a...|\n",
      "|       6|         0|        0|2020-07-01|             82|lmuhhkpyuoyslwmvX...|lmuhhkpyuoyslwmvX...|\n",
      "|       7|         0|        0|2020-07-04|             15|zoqweontumefxbgvu...|4540f452a7c4d5049...|\n",
      "|       8|         0|        0|2020-07-08|             79|sgldfgtcxufasnvsc...|sgldfgtcxufasnvsc...|\n",
      "|       9|         0|        0|2020-07-10|             25|jnykelwjjebgkwgmu...|28b93c1c62caa2b97...|\n",
      "|      10|         0|        0|2020-07-08|              8|yywjfihneygcvfnyl...|51d35e22937a5f4f2...|\n",
      "|      11|         0|        0|2020-07-01|             10|nxwejyoeznltdhcam...|000cbc89a752db6c4...|\n",
      "|      12|         0|        0|2020-07-06|             45|efmymeftivwsfljzt...|efmymeftivwsfljzt...|\n",
      "|      13|         0|        0|2020-07-10|             63|nxhvtospPhfnkavdy...|dd67ab7d952be16fa...|\n",
      "|      14|         0|        0|2020-07-03|             22|ypyusdsjzfpfbucnn...|ypyusdsjzfpfbucnn...|\n",
      "|      15|         0|        0|2020-07-09|             75|ymjvbhaxffyjcwzyn...|1ffcc4531e752f9a1...|\n",
      "|      16|         0|        0|2020-07-10|             83|phbcykkhvqsbkipwa...|phbcykkhvqsbkipwa...|\n",
      "|      17|         0|        0|2020-07-04|             54|qgnGqqnjmbqZytoug...|2e93d3b4789a5fb76...|\n",
      "|      18|         0|        0|2020-07-04|             58|ozmllbabrnhebWcex...|ozmllbabrnhebWcex...|\n",
      "|      19|         0|        0|2020-07-07|             33|kbrvXuzgiuinodtkg...|985ddbfaac225a8c2...|\n",
      "|      20|         0|        0|2020-07-09|             73|jnqjzaigjtqlfwpug...|jnqjzaigjtqlfwpug...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|hashed_bill|cnt|\n",
      "+-----------+---+\n",
      "+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashed_df.groupby(F.col(\"hashed_bill\")).agg(F.count(\"*\").alias(\"cnt\")).where(F.col(\"cnt\") > 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000040"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashed_df.select(\"hashed_bill\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|count(DISTINCT hashed_bill)|\n",
      "+---------------------------+\n",
      "|                   20000040|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashed_df.agg(F.countDistinct(F.col(\"hashed_bill\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
