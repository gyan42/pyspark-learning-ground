{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Part of the exercise we are gonna create our own dataset simulating a product selling shop. \n",
    "For the dataskewness part we are going to favour seller `0` and product `0` over other ones.\n",
    "\n",
    "\n",
    "## Products  \n",
    "\n",
    ": product_id - The product ID  \n",
    ": product_name  - The product name   \n",
    ": price - The product price   \n",
    "\n",
    "## Sellers  \n",
    "\n",
    ": seller_id  - The seller ID   \n",
    ": seller_name  - The seller name    \n",
    ": daily_target - The number of items (regardless of the product type) that the seller needs to hit his/her quota. For example, if the daily target is 100,000, the employee needs to sell 100,000 products he can hit the quota by selling 100,000 units of product_0, but also selling 30,000 units of product_1 and 70,000 units of product_2   \n",
    "\n",
    "## Sales  \n",
    "\n",
    ": order_id  - The order ID   \n",
    ": product_id  - The single product sold in the order. All orders have exactly one product)   \n",
    ": seller_id  - The selling employee ID that sold the product   \n",
    ": date - The date of the order.    \n",
    ": num_pieces_sold - The number of units sold for the specific product in the order    \n",
    ": bill_raw_text  -  A string that represents the raw text of the bill associated with the order   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bill_raw_text` is what actually control teh data size in our experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import random\n",
    "import string\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row\n",
    "from numpy.random import rand\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.9\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "findspark           1.4.2\n",
      "pyspark             2.4.7\n",
      "pytest-spark        0.6.0\n",
      "spark-nlp           2.7.3\n",
      "sparknlp            1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip list  | grep -i spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Both Pyspark and Sparn standalone should have same version**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My machine has following configuration...\n",
    "- 6 cores with 12vCores\n",
    "- 32GB RAM\n",
    "\n",
    "Spark Standalone server:\n",
    "```\n",
    "cd /opt/softwares/spark-2.4.7-bin-hadoop2.7\n",
    "\n",
    "# export your python bin path\n",
    "export PYSPARK_PYTHON=/opt/envs/ai4e/bin/python\n",
    "export PYSPARK_DRIVER_PYTHON=/opt/envs/ai4e/bin/python\n",
    "\n",
    "sbin/start-all.sh\n",
    "sbin/stop-all.sh\n",
    "```\n",
    "Spark UI: [http://localhost:8080](http://localhost:8080)   \n",
    "Spark Master URL : spark://IMCHLT276:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://IMCHLT276:7077\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.cores.max\", \"10\") \\\n",
    "    .config(\"spark.local.dir\", \"/opt/tmp/spark-temp/\") \\\n",
    "    .appName(\"DataSkewness\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# https://stackoverflow.com/questions/27774443/is-it-safe-to-temporarily-rename-tmp-and-then-create-a-tmp-symlink-to-a-differe\n",
    "# sudo mount --bind /path/to/dir/with/plenty/of/space /tmp\n",
    "# sudo lsof /tmp # check for apps\n",
    "# sudo umount /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.142:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://IMCHLT276:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataSkewness</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2d7dce0250>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Records**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the number of records as per you needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRODUCTS_COUNT = 7500#75000000 # number of reccords for product table\n",
    "SELLERS_COUNT = 10 # Numebr of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19000, 1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index = 0\n",
    "# Number of records per chunk\n",
    "chunk_size = 100 #100000 # 100 thoshand or 1 Lakh\n",
    "end_index = chunk_size\n",
    "\n",
    "PROD_ZERO_CHUNKS = 190 # Number of chunks for product zero\n",
    "OTHER_PROD_CHUNKS = 10 # Number o fchunks for other products\n",
    "\n",
    "product_zero_sales = PROD_ZERO_CHUNKS * chunk_size\n",
    "other_product_sales = OTHER_PROD_CHUNKS * chunk_size\n",
    "\n",
    "product_zero_sales, other_product_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = spark.range(PRODUCTS_COUNT).withColumnRenamed(\"id\", \"product_id\").select(F.col(\"product_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_name(product_id):\n",
    "    return f\"product_{product_id}\"\n",
    "\n",
    "get_product_name_udf = F.udf(get_product_name, StringType())\n",
    "products_df = products_df.withColumn(\"product_name\", get_product_name_udf(F.col(\"product_id\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price():\n",
    "    return random.randint(1, 150)\n",
    "get_price_udf = F.udf(get_price, IntegerType())\n",
    "\n",
    "products_df = products_df.withColumn(\"product_price\", get_price_udf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"PYSPARK_PYTHON\", 'error') == 'error':\n",
    "    print(\"Export PYSPARK_PYTHON and PYSPARK_DRIVER values and restart your jupyter!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+\n",
      "|product_id|product_name|product_price|\n",
      "+----------+------------+-------------+\n",
      "|         0|   product_0|          135|\n",
      "|         1|   product_1|            3|\n",
      "|         2|   product_2|           95|\n",
      "|         3|   product_3|           87|\n",
      "|         4|   product_4|          150|\n",
      "|         5|   product_5|           59|\n",
      "|         6|   product_6|          140|\n",
      "|         7|   product_7|          110|\n",
      "|         8|   product_8|           31|\n",
      "|         9|   product_9|          108|\n",
      "|        10|  product_10|           65|\n",
      "|        11|  product_11|          138|\n",
      "|        12|  product_12|          116|\n",
      "|        13|  product_13|           79|\n",
      "|        14|  product_14|           12|\n",
      "|        15|  product_15|           78|\n",
      "|        16|  product_16|           16|\n",
      "|        17|  product_17|          110|\n",
      "|        18|  product_18|          148|\n",
      "|        19|  product_19|           86|\n",
      "+----------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7500, None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df.count(), products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df.repartition(10).write.parquet(\"data/products_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sellers between 1 and 10\n"
     ]
    }
   ],
   "source": [
    "seller_ids = [x for x in range(1, SELLERS_COUNT)]\n",
    "print(\"Sellers between {} and {}\".format(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 44047.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+\n",
      "|seller_id|seller_name|daily_target|\n",
      "+---------+-----------+------------+\n",
      "|        0|   seller_0|     2500000|\n",
      "|        1|   seller_1|     1352975|\n",
      "|        2|   seller_2|      245478|\n",
      "|        3|   seller_3|       64451|\n",
      "|        4|   seller_4|     1567144|\n",
      "|        5|   seller_5|      588778|\n",
      "|        6|   seller_6|      525575|\n",
      "|        7|   seller_7|      480106|\n",
      "|        8|   seller_8|      304632|\n",
      "|        9|   seller_9|     1556492|\n",
      "+---------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sellers = [[0, \"seller_0\", 2500000]]\n",
    "\n",
    "for s in tqdm(seller_ids):\n",
    "    sellers.append([s, \"seller_{}\".format(s), random.randint(12000, 2000000)])\n",
    "    \n",
    "#   Save dataframe\n",
    "df = pd.DataFrame(sellers)\n",
    "df.columns = [\"seller_id\", \"seller_name\", \"daily_target\"]\n",
    "df = spark.createDataFrame(df)\n",
    "df.show()\n",
    "df.write.parquet(\"data/sellers_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare skewed data where product 0 has more entries. This means we need to create two sales dataframe and concatenate it.\n",
    "\n",
    "Due to memory constrainits on single machine, each sales dataframe is further divided into chunks and after calculation its written to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with defining the UDFs needed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2020-07-01', '2020-07-02', '2020-07-03', '2020-07-04', '2020-07-05', '2020-07-06', '2020-07-07', '2020-07-08',\n",
    "         '2020-07-09', '2020-07-10']\n",
    "def get_dates():\n",
    "    return random.choice(dates)\n",
    "\n",
    "get_dates_udf = F.udf(get_dates, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_pieces_sold():\n",
    "    return random.randint(1, 100)\n",
    "\n",
    "get_num_pieces_sold_udf = F.udf(get_num_pieces_sold, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars to choose from 26884\n"
     ]
    }
   ],
   "source": [
    "letters = string.ascii_lowercase\n",
    "letters_upper = string.ascii_uppercase\n",
    "\n",
    "for _i in range(0, 10):\n",
    "    letters += letters\n",
    "\n",
    "for _i in range(0, 10):\n",
    "    letters += letters_upper\n",
    "\n",
    "print(\"Number of chars to choose from\", len(letters))\n",
    "sample_string = random.sample(letters, 500)\n",
    "# print(\"sample_string\", ''.join(sample_string))\n",
    "\n",
    "def random_string(stringLength=200):\n",
    "    \"\"\"Generate a random string of fixed length \"\"\"\n",
    "    return ''.join(random.sample(letters, stringLength))\n",
    "\n",
    "random_string_udf = F.udf(random_string,StringType())\n",
    "\n",
    "\n",
    "def static_string():\n",
    "    \"\"\"static string of fixed length \"\"\"\n",
    "    return ''.join(sample_string)\n",
    "\n",
    "static_string_udf = F.udf(static_string, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : pass the range as input\n",
    "\n",
    "def get_product_ids():\n",
    "    return random.randint(1, PRODUCTS_COUNT)\n",
    "\n",
    "get_product_ids_udf = F.udf(get_product_ids, IntegerType())\n",
    "\n",
    "\n",
    "def get_seller_ids():\n",
    "    return random.randint(1, 10)\n",
    "\n",
    "get_seller_ids_udf = F.udf(get_seller_ids, IntegerType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining chunk size..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product zero entries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:39<00:00,  4.80it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(PROD_ZERO_CHUNKS)):\n",
    "    sales_df = spark.range(start_index, end_index).withColumnRenamed('id', 'order_id')\n",
    "    sales_df = sales_df.withColumn(\"product_id\", F.lit(0))\n",
    "    sales_df = sales_df.withColumn(\"seller_id\", F.lit(0))\n",
    "    sales_df = sales_df.withColumn(\"date\", get_dates_udf())\n",
    "    sales_df = sales_df.withColumn(\"num_pieces_sold\", get_num_pieces_sold_udf())\n",
    "    sales_df = sales_df.withColumn(\"bill_raw_text\", random_string_udf())\n",
    "    sales_df.write.parquet(\"data/sales_parquet_temp\", mode=\"append\")\n",
    "    start_index = end_index\n",
    "    end_index = end_index + chunk_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other product entries in sales DF..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.87it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(OTHER_PROD_CHUNKS)):\n",
    "    sales_df = spark.range(start_index, end_index).withColumnRenamed('id', 'order_id')\n",
    "    sales_df = sales_df.withColumn(\"product_id\", get_product_ids_udf())\n",
    "    sales_df = sales_df.withColumn(\"seller_id\", get_seller_ids_udf())\n",
    "    sales_df = sales_df.withColumn(\"date\", get_dates_udf())\n",
    "    sales_df = sales_df.withColumn(\"num_pieces_sold\", get_num_pieces_sold_udf())\n",
    "    sales_df = sales_df.withColumn(\"bill_raw_text\", random_string_udf())\n",
    "    sales_df.write.parquet(\"data/sales_parquet_temp\", mode=\"append\")\n",
    "    start_index = end_index\n",
    "    end_index = end_index + chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|   19560|      7011|        7|2020-07-07|             17|bcboaxagnoatzzcsl...|\n",
      "|   19561|      3094|        1|2020-07-02|             43|otvbjmdiwcspdkrkl...|\n",
      "|   19562|      7320|       10|2020-07-01|             43|rbttlqnsfiprszvwj...|\n",
      "|   19563|      5604|        6|2020-07-04|             54|wqlrhepzuvmiyquei...|\n",
      "|   19564|      7075|       10|2020-07-04|             43|onycjowovrmhdoejr...|\n",
      "|   19565|      2847|        3|2020-07-09|              1|rcocliwbzaumdstpk...|\n",
      "|   19566|      3725|        1|2020-07-08|             74|zeqijvkodkdvfxsfy...|\n",
      "|   19567|        11|        4|2020-07-02|             74|cltqkvaoufaSfwlgc...|\n",
      "|   19568|      6416|        7|2020-07-10|             21|tsfrzxqbsotnycqtu...|\n",
      "|   19569|      4148|        9|2020-07-06|             17|ejwqjzwiyhqmbldrb...|\n",
      "|   19430|      5987|        1|2020-07-04|             82|zxtaduyldlxilywrn...|\n",
      "|   19431|      1441|       10|2020-07-08|             70|mjneguzzsmsrynagv...|\n",
      "|   19432|       892|        9|2020-07-05|             62|dmkrxxtacypahdbzq...|\n",
      "|   19433|      1289|        5|2020-07-09|             53|oykdkmwaxfvapmhvw...|\n",
      "|   19434|      2041|       10|2020-07-06|             77|satvgoabioaqpomow...|\n",
      "|   19435|      6887|        7|2020-07-03|             54|zuqmarhnoqbtefydp...|\n",
      "|   19436|      6120|        5|2020-07-09|             95|duarqifuwyfmetzii...|\n",
      "|   19437|      2690|       10|2020-07-02|             34|lnectlldrtyichsoe...|\n",
      "|   19438|      6705|        8|2020-07-07|             84|zepviJozehdjxpwcj...|\n",
      "|   19439|      2106|        6|2020-07-01|            100|ypihcufdjozndithp...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df = spark.read.parquet(\"data/sales_parquet_temp\")\n",
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.74 ms, sys: 0 ns, total: 3.74 ms\n",
      "Wall time: 4.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sales_df.repartition(200, F.col(\"product_id\")).write.parquet(\"data/sales_parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_df=spark.createDataFrame([(\"The old brown fox\",), (\"jumps over\",), (\"the lazy log\",)], schema=[\"SampleField\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|      SampleField|\n",
      "+-----------------+\n",
      "|The old brown fox|\n",
      "|       jumps over|\n",
      "|     the lazy log|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "given_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def someNLP(text):\n",
    "    return text.split()\n",
    "\n",
    "nlp_udf = F.udf(someNLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_df.withColumn(\"new_col\", nlp_udf(F.col(\"SampleField\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}