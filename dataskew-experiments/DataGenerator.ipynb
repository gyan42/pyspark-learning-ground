{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "## Products  \n",
    "\n",
    ": product_id - The product ID  \n",
    ": product_name  - The product name   \n",
    ": price - The product price   \n",
    "\n",
    "## Sellers  \n",
    "\n",
    ": seller_id  - The seller ID   \n",
    ": seller_name  - The seller name    \n",
    ": daily_target - The number of items (regardless of the product type) that the seller needs to hit his/her quota. For example, if the daily target is 100,000, the employee needs to sell 100,000 products he can hit the quota by selling 100,000 units of product_0, but also selling 30,000 units of product_1 and 70,000 units of product_2   \n",
    "\n",
    "## Sales  \n",
    "\n",
    ": order_id  - The order ID   \n",
    ": product_id  - The single product sold in the order. All orders have exactly one product)   \n",
    ": seller_id  - The selling employee ID that sold the product   \n",
    ": date - The date of the order.    \n",
    ": num_pieces_sold - The number of units sold for the specific product in the order    \n",
    ": bill_raw_text  -  A string that represents the raw text of the bill associated with the order   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import random\n",
    "import string\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row\n",
    "from numpy.random import rand\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My machine has following configuration...\n",
    "- 6 cores with 12vCores\n",
    "- 32GB RAM\n",
    "\n",
    "Spark Standalone server:\n",
    "```\n",
    "cd /opt/softwares/spark-3.0.1-bin-hadoop3.2/\n",
    "\n",
    "# export your python bin path\n",
    "export PYSPARK_PYTHON=/opt/envs/ai4e/bin/python\n",
    "export PYSPARK_DRIVER_PYTHON=/opt/envs/ai4e/bin/python\n",
    "\n",
    "sbin/start-all.sh\n",
    "sbin/stop-all.sh\n",
    "```\n",
    "Spark UI: [http://localhost:8080](http://localhost:8080)   \n",
    "Spark Master URL : spark://IMCHLT276:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://IMCHLT276:7077\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.cores.max\", \"10\") \\\n",
    "    .config(\"spark.local.dir\", \"/opt/tmp/spark-temp/\") \\\n",
    "    .appName(\"DataSkewness\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# https://stackoverflow.com/questions/27774443/is-it-safe-to-temporarily-rename-tmp-and-then-create-a-tmp-symlink-to-a-differe\n",
    "# sudo mount --bind /path/to/dir/with/plenty/of/space /tmp\n",
    "# sudo lsof /tmp # check for apps\n",
    "# sudo umount /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.142:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://IMCHLT276:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataSkewness</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc389ca9dd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = spark.range(75000000).withColumnRenamed(\"id\", \"product_id\").select(F.col(\"product_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_name(product_id):\n",
    "    return f\"product_{product_id}\"\n",
    "\n",
    "get_product_name_udf = F.udf(get_product_name, StringType())\n",
    "products_df = products_df.withColumn(\"product_name\", get_product_name_udf(F.col(\"product_id\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price():\n",
    "    return random.randint(1, 150)\n",
    "get_price_udf = F.udf(get_price, IntegerType())\n",
    "\n",
    "products_df = products_df.withColumn(\"product_price\", get_price_udf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"PYSPARK_PYTHON\", 'error') == 'error':\n",
    "    print(\"Export PYSPARK_PYTHON and PYSPARK_DRIVER values and restsrt your jupyter!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+\n",
      "|product_id|product_name|product_price|\n",
      "+----------+------------+-------------+\n",
      "|         0|   product_0|           59|\n",
      "|         1|   product_1|           56|\n",
      "|         2|   product_2|          144|\n",
      "|         3|   product_3|            3|\n",
      "|         4|   product_4|           56|\n",
      "|         5|   product_5|          104|\n",
      "|         6|   product_6|           15|\n",
      "|         7|   product_7|           31|\n",
      "|         8|   product_8|          135|\n",
      "|         9|   product_9|          142|\n",
      "|        10|  product_10|           61|\n",
      "|        11|  product_11|           80|\n",
      "|        12|  product_12|           14|\n",
      "|        13|  product_13|           26|\n",
      "|        14|  product_14|           82|\n",
      "|        15|  product_15|          148|\n",
      "|        16|  product_16|           33|\n",
      "|        17|  product_17|          149|\n",
      "|        18|  product_18|          134|\n",
      "|        19|  product_19|           16|\n",
      "+----------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(75000000, None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df.count(), products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df.repartition(10).write.parquet(\"data/products_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sellers between 1 and 10\n"
     ]
    }
   ],
   "source": [
    "seller_ids = [x for x in range(1, 10)]\n",
    "print(\"Sellers between {} and {}\".format(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 24291.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+\n",
      "|seller_id|seller_name|daily_target|\n",
      "+---------+-----------+------------+\n",
      "|        0|   seller_0|     2500000|\n",
      "|        1|   seller_1|     1352975|\n",
      "|        2|   seller_2|      245478|\n",
      "|        3|   seller_3|       64451|\n",
      "|        4|   seller_4|     1567144|\n",
      "|        5|   seller_5|      588778|\n",
      "|        6|   seller_6|      525575|\n",
      "|        7|   seller_7|      480106|\n",
      "|        8|   seller_8|      304632|\n",
      "|        9|   seller_9|     1556492|\n",
      "+---------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sellers = [[0, \"seller_0\", 2500000]]\n",
    "\n",
    "for s in tqdm(seller_ids):\n",
    "    sellers.append([s, \"seller_{}\".format(s), random.randint(12000, 2000000)])\n",
    "    \n",
    "#   Save dataframe\n",
    "df = pd.DataFrame(sellers)\n",
    "df.columns = [\"seller_id\", \"seller_name\", \"daily_target\"]\n",
    "df = spark.createDataFrame(df)\n",
    "df.show()\n",
    "df.write.parquet(\"data/sellers_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare skewed data where product 0 has more entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2020-07-01', '2020-07-02', '2020-07-03', '2020-07-04', '2020-07-05', '2020-07-06', '2020-07-07', '2020-07-08',\n",
    "         '2020-07-09', '2020-07-10']\n",
    "def get_dates():\n",
    "    return random.choice(dates)\n",
    "\n",
    "get_dates_udf = F.udf(get_dates, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_pieces_sold():\n",
    "    return random.randint(1, 100)\n",
    "\n",
    "get_num_pieces_sold_udf = F.udf(get_num_pieces_sold, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars to choose from 26884\n",
      "sample_string brjbogvcakmjwhlzxxpdgxkmijtywakzvnhohgtenzzjksfteonrfovokxzgrudowgpbedxpwjmxmzvspapuvffmttwkusxjogzzvulocfeyfrbiusfycnhitypxtpizsvoqmimcaeSqcyzydoljvxjxodmcsmqrwdbxplbtuoagtcfzohvflrzffqockedhugrregsbxvaqvyqyubpvwtcnvbxrrvdaezolfdjdwmzaczneonhnvxwwggtjpspzpmhskcqczfpduxlkbtqnpatvnsueojmdarsqkgbgudcoajvzfMohrzrcnbzbmjkqwkhksqotlcsuwzqqiodddtqjjtlvbilbhckvrskvylgkccczhrlslavtztcnhcusgxdibrqwfcycaunftwmtlvefvameybjvlfiuqldoqarseukwemhipxawdbbkkqcerjdubyxgtyxgjbsssjhimsDlFduoopqrfrctdzgaktsmritlljrc\n"
     ]
    }
   ],
   "source": [
    "letters = string.ascii_lowercase\n",
    "letters_upper = string.ascii_uppercase\n",
    "\n",
    "for _i in range(0, 10):\n",
    "    letters += letters\n",
    "\n",
    "for _i in range(0, 10):\n",
    "    letters += letters_upper\n",
    "\n",
    "print(\"Number of chars to choose from\", len(letters))\n",
    "sample_string = random.sample(letters, 500)\n",
    "print(\"sample_string\", ''.join(sample_string))\n",
    "\n",
    "def random_string(stringLength=200):\n",
    "    \"\"\"Generate a random string of fixed length \"\"\"\n",
    "    return ''.join(random.sample(letters, stringLength))\n",
    "\n",
    "random_string_udf = F.udf(random_string,StringType())\n",
    "\n",
    "\n",
    "def static_string():\n",
    "    \"\"\"static string of fixed length \"\"\"\n",
    "    return ''.join(sample_string)\n",
    "\n",
    "static_string_udf = F.udf(static_string,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : pass the range as input\n",
    "\n",
    "def get_product_ids():\n",
    "    return random.randint(1, 75000000)\n",
    "\n",
    "get_product_ids_udf = F.udf(get_product_ids, IntegerType())\n",
    "\n",
    "\n",
    "def get_seller_ids():\n",
    "    return random.randint(1, 10)\n",
    "\n",
    "get_seller_ids_udf = F.udf(get_seller_ids, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19000000, 1000000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index = 0\n",
    "chunk_size = 100000 # 100 thoshand or 1 Lakh\n",
    "end_index = chunk_size\n",
    "\n",
    "PROD_ZERO_CHUNKS = 190 \n",
    "OTHER_PROD_CHUNKS = 10\n",
    "\n",
    "product_zero_sales = PROD_ZERO_CHUNKS * chunk_size\n",
    "other_product_sales = OTHER_PROD_CHUNKS * chunk_size\n",
    "\n",
    "product_zero_sales, other_product_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [09:43<00:00,  3.07s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(PROD_ZERO_CHUNKS)):\n",
    "    sales_df = spark.range(start_index, end_index).withColumnRenamed('id', 'order_id')\n",
    "    sales_df = sales_df.withColumn(\"product_id\", F.lit(0))\n",
    "    sales_df = sales_df.withColumn(\"seller_id\", F.lit(0))\n",
    "    sales_df = sales_df.withColumn(\"date\", get_dates_udf())\n",
    "    sales_df = sales_df.withColumn(\"num_pieces_sold\", get_num_pieces_sold_udf())\n",
    "    sales_df = sales_df.withColumn(\"bill_raw_text\", random_string_udf())\n",
    "    sales_df.write.parquet(\"data/sales_parquet_temp\", mode=\"append\")\n",
    "    start_index = end_index\n",
    "    end_index = end_index + chunk_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:31<00:00,  3.16s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(OTHER_PROD_CHUNKS)):\n",
    "    sales_df = spark.range(start_index, end_index).withColumnRenamed('id', 'order_id')\n",
    "    sales_df = sales_df.withColumn(\"product_id\", get_product_ids_udf())\n",
    "    sales_df = sales_df.withColumn(\"seller_id\", get_seller_ids_udf())\n",
    "    sales_df = sales_df.withColumn(\"date\", get_dates_udf())\n",
    "    sales_df = sales_df.withColumn(\"num_pieces_sold\", get_num_pieces_sold_udf())\n",
    "    sales_df = sales_df.withColumn(\"bill_raw_text\", random_string_udf())\n",
    "    sales_df.write.parquet(\"data/sales_parquet_temp\", mode=\"append\")\n",
    "    start_index = end_index\n",
    "    end_index = end_index + chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|19020000|  19788583|        6|2020-07-06|             90|ypbcmaeftyaxnyept...|\n",
      "|19020001|  51274713|        8|2020-07-06|             78|mtadxvnxakwnxzspm...|\n",
      "|19020002|  70376547|       10|2020-07-09|             96|xmclnwkgktnjhpzdf...|\n",
      "|19020003|  10864788|        4|2020-07-03|             47|fwayvxaxbvjjgysle...|\n",
      "|19020004|   7442520|        2|2020-07-01|             19|rYmbdxgfnzywzhboA...|\n",
      "|19020005|  27704185|        1|2020-07-02|             55|yxdamsyxqbbsgudyt...|\n",
      "|19020006|  31776189|        4|2020-07-06|              2|crnabzjyjfiabicqy...|\n",
      "|19020007|  44260415|        7|2020-07-10|             62|hJnxwncokcugfzvtn...|\n",
      "|19020008|  55961415|        5|2020-07-10|             39|jsqlemxosuzzohxjk...|\n",
      "|19020009|  57684076|        3|2020-07-09|              1|cyosounrcwonkhrpS...|\n",
      "|19020010|  17519654|        8|2020-07-03|             70|lqbogvmwjownwmubd...|\n",
      "|19020011|   9096051|        9|2020-07-06|             44|jrdufkwpovxpojglr...|\n",
      "|19020012|  68670655|        8|2020-07-10|             23|qmzkoawqpncpogfyd...|\n",
      "|19020013|  57315519|        4|2020-07-08|             12|dgasyrukiwwcjxmtR...|\n",
      "|19020014|  28982348|        4|2020-07-05|             31|tzmuavwmhlyoqbeei...|\n",
      "|19020015|  33827810|        3|2020-07-05|             78|nspegxqpzpsyvijia...|\n",
      "|19020016|  28225594|        6|2020-07-01|             24|ifmrbyevdjufaiwvv...|\n",
      "|19020017|  58850465|       10|2020-07-03|             26|tyhqnlsidttyjlfyT...|\n",
      "|19020018|  11147176|        8|2020-07-01|             37|wcvoaaqqncvrocelo...|\n",
      "|19020019|  64129665|       10|2020-07-04|             14|zwczdxcdvimmGgeiu...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df = spark.read.parquet(\"data/sales_parquet_temp\")\n",
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.8 ms, sys: 1.61 ms, total: 14.4 ms\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sales_df.repartition(200, F.col(\"product_id\")).write.parquet(\"data/sales_parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
