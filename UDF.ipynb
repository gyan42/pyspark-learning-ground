{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compound-nurse",
   "metadata": {},
   "source": [
    "# UDF\n",
    "\n",
    "https://datanoon.com/blog/pyspark_udf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "negative-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from numpy.random import rand\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.types import IntegerType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-maximum",
   "metadata": {},
   "source": [
    "My machine has following configuration...\n",
    "- 6 cores with 12vCores\n",
    "- 32GB RAM\n",
    "\n",
    "Spark Standalone server:\n",
    "```\n",
    "cd /opt/softwares/spark-3.0.1-bin-hadoop3.2/\n",
    "\n",
    "export PYSPARK_PYTHON=/opt/envs/ai4e/bin/python\n",
    "export PYSPARK_DRIVER_PYTHON=/opt/envs/ai4e/bin/python\n",
    "\n",
    "sbin/start-all.sh\n",
    "sbin/stop-all.sh\n",
    "```\n",
    "Spark UI: [http://localhost:8080](http://localhost:8080)   \n",
    "Spark Master URL : spark://IMCHLT276:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "front-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://IMCHLT276:7077\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.cores.max\", \"6\") \\\n",
    "    .config(\"spark.local.dir\", \"/opt/tmp/spark-temp/\") \\\n",
    "    .appName(\"DataSkewness\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-amendment",
   "metadata": {},
   "source": [
    "## Case 1 : Multiple invokation of UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fabulous-institute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- val1: long (nullable = true)\n",
      " |-- val2: long (nullable = true)\n",
      "\n",
      "+-----+----+----+\n",
      "| name|val1|val2|\n",
      "+-----+----+----+\n",
      "|Data1|   1|   2|\n",
      "|Data1|   2|   2|\n",
      "|Data1|   3|   6|\n",
      "|Data1|   4|   3|\n",
      "|Data1|   5|   2|\n",
      "+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Data1', 1, 2),\n",
    "       ('Data1', 2, 2),\n",
    "       ('Data1', 3, 6),\n",
    "       ('Data1', 4, 3),\n",
    "       ('Data1', 5, 2)]\n",
    "\n",
    "schema = ['name', 'val1', 'val2']\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "entire-palace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Project [pythonUDF2#94.sum AS sum#86L, pythonUDF2#94.difference AS difference#87L, pythonUDF2#94.product AS product#88L]\n",
      "+- BatchEvalPython [my_function(val1#65L, val2#66L), my_function(val1#65L, val2#66L), my_function(val1#65L, val2#66L)], [pythonUDF0#92, pythonUDF1#93, pythonUDF2#94]\n",
      "   +- *(1) Project [val1#65L, val2#66L]\n",
      "      +- *(1) Scan ExistingRDD[name#64,val1#65L,val2#66L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def my_function(arg1, arg2):\n",
    "    argsum = arg1 + arg2\n",
    "    argdiff = arg1 - arg2\n",
    "    argprod = arg1 * arg2\n",
    "    return argsum, argdiff, argprod\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField('sum', T.LongType(), False),\n",
    "    T.StructField('difference', T.LongType(), False),\n",
    "    T.StructField('product', T.LongType(), False),\n",
    "])\n",
    "\n",
    "my_function_udf = F.udf(my_function, schema)\n",
    "\n",
    "results_sdf = (\n",
    "    df\n",
    "    .select(\n",
    "        my_function_udf(\n",
    "            F.col('val1'), F.col('val2')\n",
    "        ).alias('metrics'))   # call the UDF\n",
    "    .select(F.col('metrics.*')) # expand into separate columns\n",
    ")\n",
    "\n",
    "results_sdf.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-fighter",
   "metadata": {},
   "source": [
    "Note UDF is executed three times\n",
    "\n",
    "`BatchEvalPython [my_function(val1#1L, val2#2L), my_function(val1#1L, val2#2L), my_function(val1#1L, val2#2L)], [pythonUDF0#39, pythonUDF1#40, pythonUDF2#41]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "upper-regression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+\n",
      "|sum|difference|product|\n",
      "+---+----------+-------+\n",
      "|  3|        -1|      2|\n",
      "|  4|         0|      4|\n",
      "|  9|        -3|     18|\n",
      "|  7|         1|     12|\n",
      "|  7|         3|     10|\n",
      "+---+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-amateur",
   "metadata": {},
   "source": [
    "That means that in order to do the star expansion on your metrics field, Spark will call your udf three times \n",
    "once for each item in your schema. \n",
    "\n",
    "This means you’ll be taking an already inefficient function and running it multiple times.\n",
    "\n",
    "You can trick Spark into evaluating the UDF only once by making a small change to the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "civilian-prototype",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [metrics#115.sum AS sum#117L, metrics#115.difference AS difference#118L, metrics#115.product AS product#119L]\n",
      "+- Generate explode(array(pythonUDF0#123)), false, [metrics#115]\n",
      "   +- *(2) Project [pythonUDF0#123]\n",
      "      +- BatchEvalPython [my_function(val1#65L, val2#66L)], [pythonUDF0#123]\n",
      "         +- *(1) Project [val1#65L, val2#66L]\n",
      "            +- *(1) Scan ExistingRDD[name#64,val1#65L,val2#66L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_sdf = (\n",
    "    df\n",
    "    .select(\n",
    "        F.explode(\n",
    "            F.array(\n",
    "                my_function_udf(F.col('val1'), F.col('val2'))\n",
    "            )\n",
    "        ).alias('metrics')\n",
    "    )\n",
    "    .select(F.col('metrics.*'))\n",
    ")\n",
    "results_sdf.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "promotional-vulnerability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+\n",
      "|sum|difference|product|\n",
      "+---+----------+-------+\n",
      "|  3|        -1|      2|\n",
      "|  4|         0|      4|\n",
      "|  9|        -3|     18|\n",
      "|  7|         1|     12|\n",
      "|  7|         3|     10|\n",
      "+---+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-summit",
   "metadata": {},
   "source": [
    "# Case 2 : Using NLP in UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "unauthorized-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# ! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "provincial-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-tomato",
   "metadata": {},
   "source": [
    "**Naive way of loading the spaCy model on every element**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "frequent-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=\"en_core_web_sm\"\n",
    "def spacy_tokenize(text):\n",
    "    # Note this is expensive, in practice you would use something like SpacyMagic, see footnote for link; which caches\n",
    "    # spacy.load so it isn’t happening multiple times\n",
    "    nlp = spacy.load(lang)\n",
    "    # If you are working with Python 2 and getting regular strings add x = unicode(x)\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "spacy_tokenize_udf = F.udf(spacy_tokenize, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "tropical-wedding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|# pyspark-learnin...|\n",
      "|                    |\n",
      "|PySpark refresh g...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"text\").load(\"README.md\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "official-heart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Generate explode(pythonUDF0#239), false, [col#237]\n",
      "+- *(1) Project [pythonUDF0#239]\n",
      "   +- BatchEvalPython [spacy_tokenize(value#217)], [pythonUDF0#239]\n",
      "      +- FileScan text [value#217] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex[file:/opt/vlab/gyan42/pyspark-learning-ground/README.md], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n",
      "+---------+\n",
      "|      col|\n",
      "+---------+\n",
      "|        #|\n",
      "|  pyspark|\n",
      "|        -|\n",
      "| learning|\n",
      "|        -|\n",
      "|   ground|\n",
      "|  PySpark|\n",
      "|  refresh|\n",
      "|    guide|\n",
      "|     with|\n",
      "| examples|\n",
      "|      and|\n",
      "|reference|\n",
      "|    links|\n",
      "|      for|\n",
      "|    quick|\n",
      "|      try|\n",
      "|     outs|\n",
      "|        !|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized = df.select(F.explode(spacy_tokenize_udf(F.col(\"value\"))))\n",
    "tokenized.explain()\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "informed-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy isn't serializable but loading it is semi-expensive, so cache it and reuse it for every element :)\n",
    "NLP = None\n",
    "def get_spacy_magic_for(lang):\n",
    "    global NLP\n",
    "    if NLP is None:\n",
    "        NLP = {}\n",
    "    if lang not in NLP:\n",
    "        NLP[lang] = spacy.load(lang)\n",
    "    return NLP[lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "constant-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=\"en_core_web_sm\"\n",
    "def spacy_tokenize(text):\n",
    "    # Note this is expensive, in practice you would use something like SpacyMagic, see footnote for link; which caches\n",
    "    # spacy.load so it isn’t happening multiple times\n",
    "    nlp = get_spacy_magic_for(\"en_core_web_sm\")\n",
    "    # If you are working with Python 2 and getting regular strings add x = unicode(x)\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "spacy_tokenize_udf = F.udf(spacy_tokenize, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fallen-heart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Generate explode(pythonUDF0#250), false, [col#248]\n",
      "+- *(1) Project [pythonUDF0#250]\n",
      "   +- BatchEvalPython [spacy_tokenize(value#217)], [pythonUDF0#250]\n",
      "      +- FileScan text [value#217] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex[file:/opt/vlab/gyan42/pyspark-learning-ground/README.md], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n",
      "+---------+\n",
      "|      col|\n",
      "+---------+\n",
      "|        #|\n",
      "|  pyspark|\n",
      "|        -|\n",
      "| learning|\n",
      "|        -|\n",
      "|   ground|\n",
      "|  PySpark|\n",
      "|  refresh|\n",
      "|    guide|\n",
      "|     with|\n",
      "| examples|\n",
      "|      and|\n",
      "|reference|\n",
      "|    links|\n",
      "|      for|\n",
      "|    quick|\n",
      "|      try|\n",
      "|     outs|\n",
      "|        !|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized = df.select(F.explode(spacy_tokenize_udf(F.col(\"value\"))))\n",
    "tokenized.explain()\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-spyware",
   "metadata": {},
   "source": [
    "### References\n",
    "- https://towardsdatascience.com/pyspark-udfs-and-star-expansion-b50f501dcb7b\n",
    "- https://medium.com/@manuzhang/note-about-spark-python-udf-9ddc3c162f77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-diabetes",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-journal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-flavor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "selective-johnston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Linked list is:\n",
      "2 3 1 7 \n",
      "Linked List after Deletion is:\n",
      "2 3 1 "
     ]
    }
   ],
   "source": [
    "class Node:  \n",
    "    def __init__(self, new_data):  \n",
    "        self.data = new_data  \n",
    "        self.next = None\n",
    "class LinkedList: \n",
    "    def __init__(self): \n",
    "        self.head = None\n",
    "  \n",
    "    # createNode and and make linked list  \n",
    "    def push(self, new_data):  \n",
    "        new_node = Node(new_data)  \n",
    "        new_node.next = self.head  \n",
    "        self.head = new_node  \n",
    "  \n",
    "    def deleteNode(self, n): \n",
    "        first = self.head \n",
    "        second = self.head \n",
    "        for i in range(n): \n",
    "              \n",
    "            # If count of nodes in the  \n",
    "            # given list is less than 'n' \n",
    "            if(second.next == None): \n",
    "                  \n",
    "                # If index = n then  \n",
    "                # delete the head node \n",
    "                if(i == n - 1): \n",
    "                    self.head = self.head.next\n",
    "                return self.head \n",
    "            second = second.next\n",
    "          \n",
    "        while(second.next != None): \n",
    "            second = second.next\n",
    "            first = first.next\n",
    "          \n",
    "        first.next = first.next.next\n",
    "      \n",
    "    def printList(self): \n",
    "        tmp_head = self.head \n",
    "        while(tmp_head != None): \n",
    "            print(tmp_head.data, end = ' ') \n",
    "            tmp_head = tmp_head.next\n",
    "          \n",
    "# Driver Code \n",
    "llist = LinkedList()  \n",
    "llist.push(7)  \n",
    "llist.push(1)  \n",
    "llist.push(3)  \n",
    "llist.push(2)  \n",
    "print(\"Created Linked list is:\") \n",
    "llist.printList() \n",
    "llist.deleteNode(1)  \n",
    "print(\"\\nLinked List after Deletion is:\") \n",
    "llist.printList() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "heard-funeral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linked List after Deletion is:\n",
      "2 3 "
     ]
    }
   ],
   "source": [
    "llist = LinkedList()  \n",
    "llist.push(3)\n",
    "llist.push(2)  \n",
    "llist.push(1)  \n",
    "llist.deleteNode(3)  \n",
    "print(\"\\nLinked List after Deletion is:\") \n",
    "llist.printList() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "quantitative-prior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linked List after Deletion is:\n",
      "1 "
     ]
    }
   ],
   "source": [
    "llist = LinkedList()  \n",
    "llist.push(2)  \n",
    "llist.push(1)  \n",
    "llist.deleteNode(1)  \n",
    "print(\"\\nLinked List after Deletion is:\") \n",
    "llist.printList() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unsigned-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linked List after Deletion is:\n",
      "2 "
     ]
    }
   ],
   "source": [
    "llist = LinkedList()  \n",
    "llist.push(2)  \n",
    "llist.push(1)  \n",
    "llist.deleteNode(2)  \n",
    "print(\"\\nLinked List after Deletion is:\") \n",
    "llist.printList() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "quiet-running",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linked List after Deletion is:\n"
     ]
    }
   ],
   "source": [
    "llist = LinkedList()  \n",
    "llist.push(1)  \n",
    "llist.deleteNode(1)  \n",
    "print(\"\\nLinked List after Deletion is:\") \n",
    "llist.printList() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-saudi",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
