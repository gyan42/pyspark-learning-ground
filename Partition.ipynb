{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "confident-mozambique",
   "metadata": {},
   "source": [
    "# Partition\n",
    "\n",
    "https://kontext.tech/column/spark/296/data-partitioning-in-spark-pyspark-in-depth-walkthrough  \n",
    "https://kontext.tech/column/spark/299/data-partitioning-functions-in-spark-pyspark-explained  \n",
    "https://mungingdata.com/apache-spark/partitionby/   \n",
    "https://stackoverflow.com/questions/65809909/spark-what-is-the-difference-between-repartition-and-repartitionbyrange   \n",
    "https://www.robinlinacre.com/spark_sort/    \n",
    "https://stackoverflow.com/questions/32887595/how-does-spark-achieve-sort-order/32888236#32888236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "individual-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "colored-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_CORES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "atomic-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://IMCHLT276:7077\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.cores.max\", f\"{MAX_NUM_CORES}\") \\\n",
    "    .config(\"spark.local.dir\", \"/opt/tmp/spark-temp/\") \\\n",
    "    .appName(\"DataSkewness\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cultural-tamil",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-192-168-0-142.us-west-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://IMCHLT276:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataSkewness</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f040622f4d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-thunder",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "\n",
    "**Test how partition size affects the output file numbers**\n",
    "\n",
    "**Test 1** : Number of partition is equal to the cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "robust-monster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(100000)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "entertaining-holocaust",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions() == MAX_NUM_CORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "honest-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\n",
      "part-00000-d22ab354-d3f7-4349-a366-86d0c55b321a-c000.snappy.parquet\n",
      "part-00001-d22ab354-d3f7-4349-a366-86d0c55b321a-c000.snappy.parquet\n",
      "part-00002-d22ab354-d3f7-4349-a366-86d0c55b321a-c000.snappy.parquet\n",
      "part-00003-d22ab354-d3f7-4349-a366-86d0c55b321a-c000.snappy.parquet\n",
      "part-00004-d22ab354-d3f7-4349-a366-86d0c55b321a-c000.snappy.parquet\n",
      "part-00005-d22ab354-d3f7-4349-a366-86d0c55b321a-c000.snappy.parquet\n",
      "part-00006-d22ab354-d3f7-4349-a366-86d0c55b321a-c000.snappy.parquet\n",
      "part-00007-d22ab354-d3f7-4349-a366-86d0c55b321a-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "! rm -rf /tmp/df_tes/\n",
    "df.write.parquet(\"/tmp/df_tes/\")\n",
    "!ls /tmp/df_tes/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-pantyhose",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "**Test 2** : Repartition will affect the number ouput files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "intermediate-worcester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.range(100000)\n",
    "df = df.repartition(20)\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "foreign-latex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\n",
      "part-00000-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00001-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00002-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00003-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00004-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00005-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00006-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00007-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00008-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00009-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00010-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00011-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00012-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00013-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00014-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00015-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00016-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00017-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00018-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n",
      "part-00019-d74c0121-97ea-4697-ba0a-d5bdd1e3bb13-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "! rm -rf /tmp/df_tes/\n",
    "df.write.parquet(\"/tmp/df_tes/\")\n",
    "!ls /tmp/df_tes/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-berry",
   "metadata": {},
   "source": [
    "------------------\n",
    "**Test 3** : Repartition to 1 and see what happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "downtown-annex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.range(10000000)\n",
    "df = df.repartition(1)\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "acoustic-aurora",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 39M\n",
      "drwxrwxr-x  2 mageswarand mageswarand 4.0K May  5 22:10 .\n",
      "drwxrwxrwt 37 root        root        444K May  5 22:10 ..\n",
      "-rw-r--r--  1 mageswarand mageswarand    8 May  5 22:10 ._SUCCESS.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand 306K May  5 22:10 .part-00000-8790ad36-b9eb-4e52-8373-ea0546c8221a-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand    0 May  5 22:10 _SUCCESS\n",
      "-rw-r--r--  1 mageswarand mageswarand  39M May  5 22:10 part-00000-8790ad36-b9eb-4e52-8373-ea0546c8221a-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "! rm -rf /tmp/df_tes/\n",
    "df.write.parquet(\"/tmp/df_tes/\")\n",
    "!ls /tmp/df_tes/ -alh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-fraud",
   "metadata": {},
   "source": [
    "------------------------\n",
    "**Test 4** : coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "skilled-investigation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.range(100000)\n",
    "df = df.coalesce(1)\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "exterior-nowhere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 852K\n",
      "drwxrwxr-x  2 mageswarand mageswarand 4.0K May  5 22:10 .\n",
      "drwxrwxrwt 37 root        root        444K May  5 22:10 ..\n",
      "-rw-r--r--  1 mageswarand mageswarand    8 May  5 22:10 ._SUCCESS.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand 3.1K May  5 22:10 .part-00000-a7d37985-4487-414b-935a-fe771154d105-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand    0 May  5 22:10 _SUCCESS\n",
      "-rw-r--r--  1 mageswarand mageswarand 392K May  5 22:10 part-00000-a7d37985-4487-414b-935a-fe771154d105-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "! rm -rf /tmp/df_tes/\n",
    "df.write.parquet(\"/tmp/df_tes/\")\n",
    "!ls /tmp/df_tes/ -alh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-liberia",
   "metadata": {},
   "source": [
    "------------------\n",
    "**Test 5** : Add a text column and repartition to 1 and see waht happens? Size on local disk doesn't matter. On HDFS this may change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "defined-increase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars to choose from 26884\n"
     ]
    }
   ],
   "source": [
    "import string, random\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "letters = string.ascii_lowercase\n",
    "letters_upper = string.ascii_uppercase\n",
    "\n",
    "for _i in range(0, 10):\n",
    "    letters += letters\n",
    "\n",
    "for _i in range(0, 10):\n",
    "    letters += letters_upper\n",
    "\n",
    "print(\"Number of chars to choose from\", len(letters))\n",
    "sample_string = random.sample(letters, 500)\n",
    "# print(\"sample_string\", ''.join(sample_string))\n",
    "\n",
    "def random_string(stringLength=200):\n",
    "    \"\"\"Generate a random string of fixed length \"\"\"\n",
    "    return ''.join(random.sample(letters, stringLength))\n",
    "\n",
    "random_string_udf = F.udf(random_string,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "formal-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1000000)\n",
    "df = df.withColumn(\"data\", random_string_udf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "technical-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(1, F.col(\"data\"))\n",
    "df = df.select(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "sought-turner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "alpine-soviet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.81 µs\n",
      "total 197M\n",
      "drwxrwxr-x  2 mageswarand mageswarand 4.0K May  5 22:11 .\n",
      "drwxrwxrwt 37 root        root        444K May  5 22:10 ..\n",
      "-rw-r--r--  1 mageswarand mageswarand    8 May  5 22:11 ._SUCCESS.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand 1.6M May  5 22:11 .part-00000-0f2da5c5-cc92-4a3f-8c49-2dc2512b8342-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand    0 May  5 22:11 _SUCCESS\n",
      "-rw-r--r--  1 mageswarand mageswarand 195M May  5 22:11 part-00000-0f2da5c5-cc92-4a3f-8c49-2dc2512b8342-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "! rm -rf /tmp/df_tes/\n",
    "df.write.parquet(\"/tmp/df_tes/\")\n",
    "!ls /tmp/df_tes/ -alh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-manhattan",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "paperback-public",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|SPARK_PARTITION_ID()|  count|\n",
      "+--------------------+-------+\n",
      "|                   0|1000000|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "df.groupBy(spark_partition_id()).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-influence",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "**Test 6** : Read back the stored DF with 1 partition and see how many partitions are there? Equals to number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "associate-colorado",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/tmp/df_tes/\")\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-lodging",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "**Test 7** Store as many paritions and read it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "measured-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1000000)\n",
    "df = df.withColumn(\"data\", random_string_udf())\n",
    "df = df.repartition(32, F.col(\"data\"))\n",
    "df = df.select(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "homeless-exhibition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 9.06 µs\n",
      "total 197M\n",
      "drwxrwxr-x  2 mageswarand mageswarand  12K May  5 22:12 .\n",
      "drwxrwxrwt 37 root        root        444K May  5 22:11 ..\n",
      "-rw-r--r--  1 mageswarand mageswarand    8 May  5 22:12 ._SUCCESS.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00000-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00001-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00002-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00003-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  50K May  5 22:12 .part-00004-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00005-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00006-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00007-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00008-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00009-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00010-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00011-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  50K May  5 22:12 .part-00012-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00013-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00014-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00015-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00016-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00017-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00018-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00019-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00020-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00021-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00022-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00023-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00024-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00025-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  50K May  5 22:12 .part-00026-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00027-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00028-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00029-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00030-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand  49K May  5 22:12 .part-00031-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet.crc\n",
      "-rw-r--r--  1 mageswarand mageswarand    0 May  5 22:12 _SUCCESS\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.2M May  5 22:12 part-00000-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.0M May  5 22:12 part-00001-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00002-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00003-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.2M May  5 22:12 part-00004-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00005-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.2M May  5 22:12 part-00006-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.2M May  5 22:12 part-00007-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00008-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.2M May  5 22:12 part-00009-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.2M May  5 22:12 part-00010-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00011-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.2M May  5 22:12 part-00012-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00013-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.2M May  5 22:12 part-00014-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00015-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00016-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00017-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00018-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00019-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00020-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.2M May  5 22:12 part-00021-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00022-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00023-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00024-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00025-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.2M May  5 22:12 part-00026-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00027-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00028-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00029-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.2M May  5 22:12 part-00030-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n",
      "-rw-r--r--  1 mageswarand mageswarand 6.1M May  5 22:12 part-00031-c59aa035-21bd-425f-9538-54bf009852b1-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "! rm -rf /tmp/df_tes/\n",
    "df.write.parquet(\"/tmp/df_tes/\")\n",
    "!ls /tmp/df_tes/ -alh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "violent-article",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/tmp/df_tes/\")\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "leading-tractor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|SPARK_PARTITION_ID()| count|\n",
      "+--------------------+------+\n",
      "|                   1|125740|\n",
      "|                   6|124307|\n",
      "|                   3|125104|\n",
      "|                   5|124643|\n",
      "|                   4|124935|\n",
      "|                   7|123645|\n",
      "|                   2|125380|\n",
      "|                   0|126246|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(spark_partition_id()).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-reaction",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "**Test 8** : Less number of records and more partitions? \n",
    "\n",
    "Spark will try to evenly distribute the data to each partitions. If the total partition number is greater than the actual record count (or RDD size), some partitions will be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "generous-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(10)\n",
    "df = df.withColumn(\"data\", random_string_udf())\n",
    "df = df.repartition(100, F.col(\"data\"))\n",
    "df = df.select(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "appropriate-germany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.63 µs\n",
      "_SUCCESS\n",
      "part-00000-86982ed7-5db5-4630-accd-f5e9bbfeb8b5-c000.snappy.parquet\n",
      "part-00016-86982ed7-5db5-4630-accd-f5e9bbfeb8b5-c000.snappy.parquet\n",
      "part-00017-86982ed7-5db5-4630-accd-f5e9bbfeb8b5-c000.snappy.parquet\n",
      "part-00021-86982ed7-5db5-4630-accd-f5e9bbfeb8b5-c000.snappy.parquet\n",
      "part-00028-86982ed7-5db5-4630-accd-f5e9bbfeb8b5-c000.snappy.parquet\n",
      "part-00044-86982ed7-5db5-4630-accd-f5e9bbfeb8b5-c000.snappy.parquet\n",
      "part-00054-86982ed7-5db5-4630-accd-f5e9bbfeb8b5-c000.snappy.parquet\n",
      "part-00089-86982ed7-5db5-4630-accd-f5e9bbfeb8b5-c000.snappy.parquet\n",
      "part-00094-86982ed7-5db5-4630-accd-f5e9bbfeb8b5-c000.snappy.parquet\n",
      "part-00099-86982ed7-5db5-4630-accd-f5e9bbfeb8b5-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "! rm -rf /tmp/df_tes/\n",
    "df.write.parquet(\"/tmp/df_tes/\")\n",
    "!ls /tmp/df_tes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "missing-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df.groupBy(spark_partition_id()).agg(F.count(\"data\").alias(\"id\")).orderBy(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "elect-palestinian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|SPARK_PARTITION_ID()| id|\n",
      "+--------------------+---+\n",
      "|                  92|  1|\n",
      "|                  34|  1|\n",
      "|                  11|  1|\n",
      "|                   3|  1|\n",
      "|                  82|  1|\n",
      "|                  45|  1|\n",
      "|                  86|  1|\n",
      "|                  16|  1|\n",
      "|                  57|  1|\n",
      "|                  60|  1|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "pacific-yield",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-stretch",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "**Test 9** Default column repartition? Equals to 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "conventional-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(10000)\n",
    "df = df.withColumn(\"data\", random_string_udf())\n",
    "df = df.repartition(F.col(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "close-coaching",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 6.44 µs\n",
      "201\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "! rm -rf /tmp/df_tes/\n",
    "df.write.parquet(\"/tmp/df_tes/\")\n",
    "!ls /tmp/df_tes/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "handled-syndication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(spark_partition_id()).count().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-globe",
   "metadata": {},
   "source": [
    "---------------\n",
    "**Test 10** Multi column parition and write partition keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "genetic-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "least-thomas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|Country|      Date|Amount|\n",
      "+-------+----------+------+\n",
      "|     CN|2019-01-01|    10|\n",
      "|     AU|2019-01-01|    10|\n",
      "|     CN|2019-01-02|    11|\n",
      "|     AU|2019-01-02|    11|\n",
      "|     CN|2019-01-03|    12|\n",
      "|     AU|2019-01-03|    12|\n",
      "|     CN|2019-01-04|    13|\n",
      "|     AU|2019-01-04|    13|\n",
      "|     CN|2019-01-05|    14|\n",
      "|     AU|2019-01-05|    14|\n",
      "|     CN|2019-01-06|    15|\n",
      "|     AU|2019-01-06|    15|\n",
      "|     CN|2019-01-07|    16|\n",
      "|     AU|2019-01-07|    16|\n",
      "|     CN|2019-01-08|    17|\n",
      "|     AU|2019-01-08|    17|\n",
      "|     CN|2019-01-09|    18|\n",
      "|     AU|2019-01-09|    18|\n",
      "|     CN|2019-01-10|    19|\n",
      "|     AU|2019-01-10|    19|\n",
      "+-------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "start_date = date(2019, 1, 1)\n",
    "data = []\n",
    "for i in range(0, 50):\n",
    "    data.append({\"Country\": \"CN\", \"Date\": start_date + timedelta(days=i), \"Amount\": 10+i})\n",
    "    data.append({\"Country\": \"AU\", \"Date\": start_date + timedelta(days=i), \"Amount\": 10+i})\n",
    "\n",
    "schema = StructType([StructField('Country', StringType(), nullable=False),\n",
    "                     StructField('Date', DateType(), nullable=False),\n",
    "                     StructField('Amount', IntegerType(), nullable=False)])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()\n",
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "binding-surface",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "+-------+----------+------+----+-----+---+\n",
      "|Country|      Date|Amount|Year|Month|Day|\n",
      "+-------+----------+------+----+-----+---+\n",
      "|     AU|2019-01-21|    30|2019|    1| 21|\n",
      "|     CN|2019-01-29|    38|2019|    1| 29|\n",
      "|     AU|2019-01-19|    28|2019|    1| 19|\n",
      "|     AU|2019-02-07|    47|2019|    2|  7|\n",
      "|     AU|2019-02-02|    42|2019|    2|  2|\n",
      "|     AU|2019-02-05|    45|2019|    2|  5|\n",
      "|     AU|2019-02-08|    48|2019|    2|  8|\n",
      "|     CN|2019-01-27|    36|2019|    1| 27|\n",
      "|     CN|2019-01-21|    30|2019|    1| 21|\n",
      "|     AU|2019-01-11|    20|2019|    1| 11|\n",
      "|     CN|2019-01-25|    34|2019|    1| 25|\n",
      "|     CN|2019-02-06|    46|2019|    2|  6|\n",
      "|     CN|2019-01-19|    28|2019|    1| 19|\n",
      "|     CN|2019-02-19|    59|2019|    2| 19|\n",
      "|     AU|2019-02-03|    43|2019|    2|  3|\n",
      "|     AU|2019-02-09|    49|2019|    2|  9|\n",
      "|     CN|2019-01-14|    23|2019|    1| 14|\n",
      "|     AU|2019-01-16|    25|2019|    1| 16|\n",
      "|     CN|2019-02-16|    56|2019|    2| 16|\n",
      "|     AU|2019-01-10|    19|2019|    1| 10|\n",
      "+-------+----------+------+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Year\", F.year(\"Date\")).withColumn(\"Month\", F.month(\"Date\")).withColumn(\"Day\", F.dayofmonth(\"Date\"))\n",
    "df = df.repartition(\"Year\", \"Month\", \"Day\", \"Country\")\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bound-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"Year\", \"Month\", \"Day\", \"Country\").mode(\"overwrite\").csv(\"/tmp/df_tes/\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "velvet-finish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/tmp/df_tes/\u001b[00m\n",
      "├── \u001b[01;34mYear=2019\u001b[00m\n",
      "│   ├── \u001b[01;34mMonth=1\u001b[00m\n",
      "│   │   ├── \u001b[01;34mDay=1\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00151-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00172-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=10\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00037-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00112-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=11\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00026-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00111-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=12\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00060-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00060-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=13\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00039-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00150-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=14\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00170-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00033-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=15\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00104-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00138-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=16\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00033-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00106-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=17\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00135-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00173-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=18\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00147-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00173-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=19\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00017-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00027-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=2\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00046-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00104-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=20\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00125-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00180-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=21\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00000-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00025-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=22\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00041-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00081-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=23\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00078-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00125-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=24\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00132-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00072-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=25\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00076-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00026-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=26\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00119-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00161-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=27\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00114-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00024-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=28\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00157-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00052-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=29\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00038-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00005-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=3\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00046-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00192-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=30\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00175-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00071-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=31\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00084-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00068-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=4\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00115-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00099-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=5\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00067-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00110-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=6\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00066-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00132-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=7\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00096-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00167-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   ├── \u001b[01;34mDay=8\u001b[00m\n",
      "│   │   │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │   │   │   └── part-00151-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │   │       └── part-00082-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │   └── \u001b[01;34mDay=9\u001b[00m\n",
      "│   │       ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│   │       │   └── part-00174-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   │       └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│   │           └── part-00129-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│   └── \u001b[01;34mMonth=2\u001b[00m\n",
      "│       ├── \u001b[01;34mDay=1\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00103-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00053-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       ├── \u001b[01;34mDay=10\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00152-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00127-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       ├── \u001b[01;34mDay=11\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00063-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00104-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       ├── \u001b[01;34mDay=12\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00158-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00132-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       ├── \u001b[01;34mDay=13\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00074-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00132-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       ├── \u001b[01;34mDay=14\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00149-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00105-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       ├── \u001b[01;34mDay=15\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00177-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00114-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       ├── \u001b[01;34mDay=16\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00062-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00036-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       ├── \u001b[01;34mDay=17\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00111-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00087-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       ├── \u001b[01;34mDay=18\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00138-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00085-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       ├── \u001b[01;34mDay=19\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00041-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00027-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       ├── \u001b[01;34mDay=2\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00018-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00168-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       ├── \u001b[01;34mDay=3\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00031-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00104-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       ├── \u001b[01;34mDay=4\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00134-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00153-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       ├── \u001b[01;34mDay=5\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00021-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00153-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       ├── \u001b[01;34mDay=6\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00054-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00026-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       ├── \u001b[01;34mDay=7\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00018-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00165-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       ├── \u001b[01;34mDay=8\u001b[00m\n",
      "│       │   ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│       │   │   └── part-00021-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       │   └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│       │       └── part-00184-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│       └── \u001b[01;34mDay=9\u001b[00m\n",
      "│           ├── \u001b[01;34mCountry=AU\u001b[00m\n",
      "│           │   └── part-00032-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "│           └── \u001b[01;34mCountry=CN\u001b[00m\n",
      "│               └── part-00146-551dc6ed-c92d-4d61-914b-b874ce71b437.c000.csv\n",
      "└── _SUCCESS\n",
      "\n",
      "153 directories, 101 files\n"
     ]
    }
   ],
   "source": [
    "!tree /tmp/df_tes/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-titanium",
   "metadata": {},
   "source": [
    "**Read from partitioned data**\n",
    "\n",
    "Now let’s read the data from the partitioned files with the these criteria:\n",
    "\n",
    "    Year= 2019\n",
    "    Month=2\n",
    "    Day=1\n",
    "    Country=CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "numerous-solid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "+----------+------+\n",
      "|       _c0|   _c1|\n",
      "+----------+------+\n",
      "|      Date|Amount|\n",
      "|2019-02-01|    41|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/tmp/df_tes/Year=2019/Month=2/Day=1/Country=CN\")\n",
    "print(df.rdd.getNumPartitions()) # only one becaise there is only one record\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-peoples",
   "metadata": {},
   "source": [
    "Similarly, we can also query all the data for the second month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "american-waste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "+----------+------+---+-------+\n",
      "|       _c0|   _c1|Day|Country|\n",
      "+----------+------+---+-------+\n",
      "|      Date|Amount|  3|     CN|\n",
      "|2019-02-03|    43|  3|     CN|\n",
      "|      Date|Amount| 10|     CN|\n",
      "|2019-02-10|    50| 10|     CN|\n",
      "|      Date|Amount| 13|     CN|\n",
      "|2019-02-13|    53| 13|     CN|\n",
      "|      Date|Amount| 16|     AU|\n",
      "|2019-02-16|    56| 16|     AU|\n",
      "|      Date|Amount| 15|     CN|\n",
      "|2019-02-15|    55| 15|     CN|\n",
      "|      Date|Amount| 16|     CN|\n",
      "|2019-02-16|    56| 16|     CN|\n",
      "|      Date|Amount| 17|     CN|\n",
      "|2019-02-17|    57| 17|     CN|\n",
      "|      Date|Amount| 10|     AU|\n",
      "|2019-02-10|    50| 10|     AU|\n",
      "|      Date|Amount|  5|     AU|\n",
      "|2019-02-05|    45|  5|     AU|\n",
      "|      Date|Amount| 15|     AU|\n",
      "|2019-02-15|    55| 15|     AU|\n",
      "+----------+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/tmp/df_tes/Year=2019/Month=2\")\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-remains",
   "metadata": {},
   "source": [
    "**Use wildcards for partition discovery**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "pending-costume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "+----------+------+----+-----+---+-------+\n",
      "|       _c0|   _c1|Year|Month|Day|Country|\n",
      "+----------+------+----+-----+---+-------+\n",
      "|      Date|Amount|2019|    2|  3|     CN|\n",
      "|2019-02-03|    43|2019|    2|  3|     CN|\n",
      "|      Date|Amount|2019|    1| 17|     CN|\n",
      "|2019-01-17|    26|2019|    1| 17|     CN|\n",
      "|      Date|Amount|2019|    2| 10|     CN|\n",
      "|2019-02-10|    50|2019|    2| 10|     CN|\n",
      "|      Date|Amount|2019|    1|  3|     CN|\n",
      "|2019-01-03|    12|2019|    1|  3|     CN|\n",
      "|      Date|Amount|2019|    1| 24|     CN|\n",
      "|2019-01-24|    33|2019|    1| 24|     CN|\n",
      "|      Date|Amount|2019|    2| 13|     CN|\n",
      "|2019-02-13|    53|2019|    2| 13|     CN|\n",
      "|      Date|Amount|2019|    1| 25|     CN|\n",
      "|2019-01-25|    34|2019|    1| 25|     CN|\n",
      "|      Date|Amount|2019|    1|  1|     CN|\n",
      "|2019-01-01|    10|2019|    1|  1|     CN|\n",
      "|      Date|Amount|2019|    1| 21|     CN|\n",
      "|2019-01-21|    30|2019|    1| 21|     CN|\n",
      "|      Date|Amount|2019|    2| 15|     CN|\n",
      "|2019-02-15|    55|2019|    2| 15|     CN|\n",
      "+----------+------+----+-----+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"basePath\", \"/tmp/df_tes/\").csv(\"/tmp/df_tes/Year=*/Month=*/Day=*/Country=CN\")\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-football",
   "metadata": {},
   "source": [
    "We can use wildcards in any part of the path for partition discovery. For example, the following code looks data for month 2 of Country AU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "strong-terminology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "+----------+------+----+-----+---+-------+\n",
      "|       _c0|   _c1|Year|Month|Day|Country|\n",
      "+----------+------+----+-----+---+-------+\n",
      "|      Date|Amount|2019|    2| 16|     AU|\n",
      "|2019-02-16|    56|2019|    2| 16|     AU|\n",
      "|      Date|Amount|2019|    2| 10|     AU|\n",
      "|2019-02-10|    50|2019|    2| 10|     AU|\n",
      "|      Date|Amount|2019|    2|  5|     AU|\n",
      "|2019-02-05|    45|2019|    2|  5|     AU|\n",
      "|      Date|Amount|2019|    2| 15|     AU|\n",
      "|2019-02-15|    55|2019|    2| 15|     AU|\n",
      "|      Date|Amount|2019|    2| 12|     AU|\n",
      "|2019-02-12|    52|2019|    2| 12|     AU|\n",
      "|      Date|Amount|2019|    2|  1|     AU|\n",
      "|2019-02-01|    41|2019|    2|  1|     AU|\n",
      "|      Date|Amount|2019|    2|  8|     AU|\n",
      "|2019-02-08|    48|2019|    2|  8|     AU|\n",
      "|      Date|Amount|2019|    2|  6|     AU|\n",
      "|2019-02-06|    46|2019|    2|  6|     AU|\n",
      "|      Date|Amount|2019|    2| 14|     AU|\n",
      "|2019-02-14|    54|2019|    2| 14|     AU|\n",
      "|      Date|Amount|2019|    2| 13|     AU|\n",
      "|2019-02-13|    53|2019|    2| 13|     AU|\n",
      "+----------+------+----+-----+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"basePath\", \"/tmp/df_tes/\").csv(\"/tmp/df_tes/Year=*/Month=2/Day=*/Country=AU\")\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-newport",
   "metadata": {},
   "source": [
    "## Data Partitioning Functions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "wrapped-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.rdd import portable_hash\n",
    "from pyspark import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "elect-equipment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/envs/ai4e/lib/python3.7/site-packages/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+\n",
      "|Amount|Country| ID|\n",
      "+------+-------+---+\n",
      "|    11|     AU|  1|\n",
      "|    12|     US|  2|\n",
      "|    13|     CN|  3|\n",
      "|    14|     AU|  4|\n",
      "|    15|     US|  5|\n",
      "|    16|     CN|  6|\n",
      "|    17|     AU|  7|\n",
      "|    18|     US|  8|\n",
      "|    19|     CN|  9|\n",
      "|    20|     AU| 10|\n",
      "|    21|     US| 11|\n",
      "|    22|     CN| 12|\n",
      "+------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Populate sample data\n",
    "countries = (\"CN\", \"AU\", \"US\")\n",
    "data = []\n",
    "for i in range(1, 13):\n",
    "    data.append({\"ID\": i, \"Country\": countries[i % 3],  \"Amount\": 10+i})\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "former-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_partitions(df):\n",
    "    numPartitions = df.rdd.getNumPartitions()\n",
    "    print(\"Total partitions: {}\\n\".format(numPartitions))\n",
    "    print(\"Partitioner: {}\\n\".format(df.rdd.partitioner))\n",
    "    df.explain()\n",
    "    print(\"\\n\")\n",
    "    parts = df.rdd.glom().collect()\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for p in parts:\n",
    "        print(\"\\nPartition {}:\".format(i))\n",
    "        for r in p:\n",
    "            print(\"Row {}:{}\".format(j, r))\n",
    "            j = j+1\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "rational-found",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 8\n",
      "\n",
      "Partitioner: None\n",
      "\n",
      "== Physical Plan ==\n",
      "Scan ExistingRDD[Amount#796L,Country#797,ID#798L]\n",
      "\n",
      "\n",
      "\n",
      "Partition 0:\n",
      "Row 0:Row(Amount=11, Country='AU', ID=1)\n",
      "\n",
      "Partition 1:\n",
      "Row 1:Row(Amount=12, Country='US', ID=2)\n",
      "Row 2:Row(Amount=13, Country='CN', ID=3)\n",
      "\n",
      "Partition 2:\n",
      "Row 3:Row(Amount=14, Country='AU', ID=4)\n",
      "\n",
      "Partition 3:\n",
      "Row 4:Row(Amount=15, Country='US', ID=5)\n",
      "Row 5:Row(Amount=16, Country='CN', ID=6)\n",
      "\n",
      "Partition 4:\n",
      "Row 6:Row(Amount=17, Country='AU', ID=7)\n",
      "\n",
      "Partition 5:\n",
      "Row 7:Row(Amount=18, Country='US', ID=8)\n",
      "Row 8:Row(Amount=19, Country='CN', ID=9)\n",
      "\n",
      "Partition 6:\n",
      "Row 9:Row(Amount=20, Country='AU', ID=10)\n",
      "\n",
      "Partition 7:\n",
      "Row 10:Row(Amount=21, Country='US', ID=11)\n",
      "Row 11:Row(Amount=22, Country='CN', ID=12)\n"
     ]
    }
   ],
   "source": [
    "print_partitions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "proud-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(3, \"Country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "chief-forum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 3\n",
      "\n",
      "Partitioner: None\n",
      "\n",
      "== Physical Plan ==\n",
      "Exchange hashpartitioning(Country#797, 3)\n",
      "+- Scan ExistingRDD[Amount#796L,Country#797,ID#798L]\n",
      "\n",
      "\n",
      "\n",
      "Partition 0:\n",
      "\n",
      "Partition 1:\n",
      "Row 0:Row(Amount=15, Country='US', ID=5)\n",
      "Row 1:Row(Amount=16, Country='CN', ID=6)\n",
      "Row 2:Row(Amount=21, Country='US', ID=11)\n",
      "Row 3:Row(Amount=22, Country='CN', ID=12)\n",
      "Row 4:Row(Amount=12, Country='US', ID=2)\n",
      "Row 5:Row(Amount=13, Country='CN', ID=3)\n",
      "Row 6:Row(Amount=18, Country='US', ID=8)\n",
      "Row 7:Row(Amount=19, Country='CN', ID=9)\n",
      "\n",
      "Partition 2:\n",
      "Row 8:Row(Amount=14, Country='AU', ID=4)\n",
      "Row 9:Row(Amount=20, Country='AU', ID=10)\n",
      "Row 10:Row(Amount=11, Country='AU', ID=1)\n",
      "Row 11:Row(Amount=17, Country='AU', ID=7)\n"
     ]
    }
   ],
   "source": [
    "print_partitions(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-lender",
   "metadata": {},
   "source": [
    "**You may expect that each partition includes data for each Country but that is not the case. Why? Because repartition function by default uses hash partitioning. For different country code, it may be allocated into the same partition number.**\n",
    "We can verify this by using the following code to calculate the hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "liquid-windows",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+--------------------+----------+\n",
      "|Amount|Country| ID|               Hash#|Partition#|\n",
      "+------+-------+---+--------------------+----------+\n",
      "|    12|     US|  2|-8328537658613580243|      -1.0|\n",
      "|    13|     CN|  3|-7458853143580063552|      -1.0|\n",
      "|    18|     US|  8|-8328537658613580243|      -1.0|\n",
      "|    19|     CN|  9|-7458853143580063552|      -1.0|\n",
      "|    15|     US|  5|-8328537658613580243|      -1.0|\n",
      "|    16|     CN|  6|-7458853143580063552|      -1.0|\n",
      "|    21|     US| 11|-8328537658613580243|      -1.0|\n",
      "|    22|     CN| 12|-7458853143580063552|      -1.0|\n",
      "|    14|     AU|  4| 6593628092971972691|       0.0|\n",
      "|    20|     AU| 10| 6593628092971972691|       0.0|\n",
      "|    11|     AU|  1| 6593628092971972691|       0.0|\n",
      "|    17|     AU|  7| 6593628092971972691|       0.0|\n",
      "+------+-------+---+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf_portable_hash = F.udf(lambda str: portable_hash(str))\n",
    "df = df.withColumn(\"Hash#\", udf_portable_hash(df.Country))\n",
    "df = df.withColumn(\"Partition#\", df[\"Hash#\"] % 3)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-crime",
   "metadata": {},
   "source": [
    "The output shows that each country’s data is now located in the same partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "royal-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = (\"CN\", \"AU\", \"US\")\n",
    "def country_partitioning(k):\n",
    "    return countries.index(k)\n",
    "    \n",
    "udf_country_hash = F.udf(lambda str: country_partitioning(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "educational-seating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+-----+----------+\n",
      "|Amount|Country| ID|Hash#|Partition#|\n",
      "+------+-------+---+-----+----------+\n",
      "|    17|     AU|  7|    1|       1.0|\n",
      "|    14|     AU|  4|    1|       1.0|\n",
      "|    20|     AU| 10|    1|       1.0|\n",
      "|    11|     AU|  1|    1|       1.0|\n",
      "|    22|     CN| 12|    0|       0.0|\n",
      "|    16|     CN|  6|    0|       0.0|\n",
      "|    19|     CN|  9|    0|       0.0|\n",
      "|    13|     CN|  3|    0|       0.0|\n",
      "|    15|     US|  5|    2|       2.0|\n",
      "|    12|     US|  2|    2|       2.0|\n",
      "|    18|     US|  8|    2|       2.0|\n",
      "|    21|     US| 11|    2|       2.0|\n",
      "+------+-------+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numPartitions = 3\n",
    "# df = df.partitionBy(numPartitions, country_partitioning)\n",
    "df = df.withColumn(\"Hash#\", udf_country_hash(df['Country']))\n",
    "df = df.withColumn(\"Partition#\", df[\"Hash#\"] % numPartitions)\n",
    "df.orderBy('Country').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "stuck-snake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 3\n",
      "\n",
      "Partitioner: None\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [Amount#796L, Country#797, ID#798L, pythonUDF1#874 AS Hash##843, (cast(pythonUDF1#874 as double) % 3.0) AS Partition##849]\n",
      "+- BatchEvalPython [<lambda>(Country#797), <lambda>(Country#797)], [Amount#796L, Country#797, ID#798L, pythonUDF0#873, pythonUDF1#874]\n",
      "   +- Exchange hashpartitioning(Country#797, 3)\n",
      "      +- Scan ExistingRDD[Amount#796L,Country#797,ID#798L]\n",
      "\n",
      "\n",
      "\n",
      "Partition 0:\n",
      "\n",
      "Partition 1:\n",
      "Row 0:Row(Amount=15, Country='US', ID=5, Hash#='2', Partition#=2.0)\n",
      "Row 1:Row(Amount=16, Country='CN', ID=6, Hash#='0', Partition#=0.0)\n",
      "Row 2:Row(Amount=21, Country='US', ID=11, Hash#='2', Partition#=2.0)\n",
      "Row 3:Row(Amount=22, Country='CN', ID=12, Hash#='0', Partition#=0.0)\n",
      "Row 4:Row(Amount=12, Country='US', ID=2, Hash#='2', Partition#=2.0)\n",
      "Row 5:Row(Amount=13, Country='CN', ID=3, Hash#='0', Partition#=0.0)\n",
      "Row 6:Row(Amount=18, Country='US', ID=8, Hash#='2', Partition#=2.0)\n",
      "Row 7:Row(Amount=19, Country='CN', ID=9, Hash#='0', Partition#=0.0)\n",
      "\n",
      "Partition 2:\n",
      "Row 8:Row(Amount=14, Country='AU', ID=4, Hash#='1', Partition#=1.0)\n",
      "Row 9:Row(Amount=20, Country='AU', ID=10, Hash#='1', Partition#=1.0)\n",
      "Row 10:Row(Amount=11, Country='AU', ID=1, Hash#='1', Partition#=1.0)\n",
      "Row 11:Row(Amount=17, Country='AU', ID=7, Hash#='1', Partition#=1.0)\n"
     ]
    }
   ],
   "source": [
    "print_partitions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "arctic-girlfriend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 3\n",
      "\n",
      "Partitioner: None\n",
      "\n",
      "== Physical Plan ==\n",
      "Exchange hashpartitioning(Partition##849, 3)\n",
      "+- *(1) Project [Amount#796L, Country#797, ID#798L, pythonUDF1#876 AS Hash##843, (cast(pythonUDF1#876 as double) % 3.0) AS Partition##849]\n",
      "   +- BatchEvalPython [<lambda>(Country#797), <lambda>(Country#797)], [Amount#796L, Country#797, ID#798L, pythonUDF0#875, pythonUDF1#876]\n",
      "      +- Exchange hashpartitioning(Country#797, 3)\n",
      "         +- Scan ExistingRDD[Amount#796L,Country#797,ID#798L]\n",
      "\n",
      "\n",
      "\n",
      "Partition 0:\n",
      "Row 0:Row(Amount=12, Country='US', ID=2, Hash#='2', Partition#=2.0)\n",
      "Row 1:Row(Amount=18, Country='US', ID=8, Hash#='2', Partition#=2.0)\n",
      "Row 2:Row(Amount=15, Country='US', ID=5, Hash#='2', Partition#=2.0)\n",
      "Row 3:Row(Amount=21, Country='US', ID=11, Hash#='2', Partition#=2.0)\n",
      "\n",
      "Partition 1:\n",
      "Row 4:Row(Amount=13, Country='CN', ID=3, Hash#='0', Partition#=0.0)\n",
      "Row 5:Row(Amount=19, Country='CN', ID=9, Hash#='0', Partition#=0.0)\n",
      "Row 6:Row(Amount=16, Country='CN', ID=6, Hash#='0', Partition#=0.0)\n",
      "Row 7:Row(Amount=22, Country='CN', ID=12, Hash#='0', Partition#=0.0)\n",
      "\n",
      "Partition 2:\n",
      "Row 8:Row(Amount=11, Country='AU', ID=1, Hash#='1', Partition#=1.0)\n",
      "Row 9:Row(Amount=17, Country='AU', ID=7, Hash#='1', Partition#=1.0)\n",
      "Row 10:Row(Amount=14, Country='AU', ID=4, Hash#='1', Partition#=1.0)\n",
      "Row 11:Row(Amount=20, Country='AU', ID=10, Hash#='1', Partition#=1.0)\n"
     ]
    }
   ],
   "source": [
    "print_partitions(df.repartition(3, \"Partition#\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-church",
   "metadata": {},
   "source": [
    "# Range Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "textile-guard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "|    5|\n",
      "|    6|\n",
      "|    7|\n",
      "|    8|\n",
      "|    9|\n",
      "|   10|\n",
      "|   11|\n",
      "|   12|\n",
      "|   13|\n",
      "|   14|\n",
      "|   15|\n",
      "|   16|\n",
      "|   17|\n",
      "|   18|\n",
      "|   19|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df_test_1 = [i for i in range(10000)]\n",
    "df_test_1 = spark.createDataFrame(df_test_1, schema=IntegerType())\n",
    "df_test_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "several-ministry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_2 = [0 for i in range(10000)] + [500, 1000, 10000]\n",
    "df_test_2 = spark.createDataFrame(df_test_2, schema=IntegerType())\n",
    "df_test_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-attitude",
   "metadata": {},
   "source": [
    "`repartition` applies the `HashPartitioner` when one or more columns are provided and the `RoundRobinPartitioner` which distributes the data evenly across the provided number of partitions. If one column (or more) is provided, those values will be hashed and used to determine the partition number by calculating something like `partition = hash(columns) % numberOfPartitions`.\n",
    "\n",
    "`repartitionByRange` will partition the data based on a range of the column values. This is usually used for continuous (not discrete) values such as any kind of numbers. `Note that due to performance reasons this method uses sampling to estimate the ranges`. Hence, the output may not be consistent, since sampling can return different values. The sample size can be controlled by the config `spark.sql.execution.rangeExchange.sampleSizePerPartition`.\n",
    "\n",
    "It is also worth mentioning that for both methods if no `numPartitions` is given, by default it partitions the Dataframe data into `spark.sql.shuffle.partitions` configured in your Spark session, and could be coalesced by Adaptive Query Execution (available since Spark 3.x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "excellent-tsunami",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---------+---------+\n",
      "|partition|count|min_value|max_value|\n",
      "+---------+-----+---------+---------+\n",
      "|        0| 1024|        0|     1023|\n",
      "|        1| 1024|     1024|     2047|\n",
      "|        2| 1024|     2048|     3071|\n",
      "|        3| 2048|     3072|     5119|\n",
      "|        4| 1024|     5120|     6143|\n",
      "|        5| 1024|     6144|     7167|\n",
      "|        6| 1024|     7168|     8191|\n",
      "|        7| 1808|     8192|     9999|\n",
      "+---------+-----+---------+---------+\n",
      "\n",
      "+---------+-----+---------+---------+\n",
      "|partition|count|min_value|max_value|\n",
      "+---------+-----+---------+---------+\n",
      "|        0| 1024|        0|        0|\n",
      "|        1| 1024|        0|        0|\n",
      "|        2| 1024|        0|        0|\n",
      "|        3| 2048|        0|        0|\n",
      "|        4| 1024|        0|        0|\n",
      "|        5| 1024|        0|        0|\n",
      "|        6| 1024|        0|        0|\n",
      "|        7| 1811|        0|    10000|\n",
      "+---------+-----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F# spark_partition_id\n",
    "# applying SQL built-in function to determin actual partition\n",
    "\n",
    "def get_partition_info(df):\n",
    "    test_res_df = df\\\n",
    "        .withColumn(\"partition\", F.spark_partition_id()) \\\n",
    "        .groupBy(F.col(\"partition\"))\\\n",
    "        .agg(F.count(F.col(\"value\")).alias(\"count\"),\\\n",
    "          F.min(F.col(\"value\")).alias(\"min_value\"),\\\n",
    "          F.max(F.col(\"value\")).alias(\"max_value\"))\\\n",
    "        .orderBy(F.col(\"partition\"))\n",
    "\n",
    "    test_res_df.show()\n",
    "    \n",
    "get_partition_info(df_test_1)\n",
    "get_partition_info(df_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-intro",
   "metadata": {},
   "source": [
    "As expected, we get 4 partitions and because the values of df are ranging from 0 to 1000000 we see that their hashed values will result in a well distributed Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "inner-maldives",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---------+---------+\n",
      "|partition|count|min_value|max_value|\n",
      "+---------+-----+---------+---------+\n",
      "|        0| 2490|       12|     9991|\n",
      "|        1| 2518|        6|     9999|\n",
      "|        2| 2507|        2|     9997|\n",
      "|        3| 2485|        0|     9992|\n",
      "+---------+-----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_partition_info(df_test_1.repartition(4, F.col(\"value\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-performer",
   "metadata": {},
   "source": [
    "Also in this case, we get 4 partitions but this time the min and max values clearly shows the ranges of values within a partition. It is almost equally distributed with 250000 values per partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "judicial-contrast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---------+---------+\n",
      "|partition|count|min_value|max_value|\n",
      "+---------+-----+---------+---------+\n",
      "|        0| 2515|        0|     2514|\n",
      "|        1| 2462|     2515|     4976|\n",
      "|        2| 2510|     4977|     7486|\n",
      "|        3| 2513|     7487|     9999|\n",
      "+---------+-----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_partition_info(df_test_1.repartitionByRange(4, F.col(\"value\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-ground",
   "metadata": {},
   "source": [
    "Now, we are using the other Dataframe df_test_2. Here, the hashing algorithm is hashing the values which are only 0, 5000, 10000 or 100000. Of course, the hash of the value 0 will always be the same, so all Zeros end up in the same partition (in this case partition 3). The other two partitions only contain one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "facial-upset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---------+---------+\n",
      "|partition|count|min_value|max_value|\n",
      "+---------+-----+---------+---------+\n",
      "|        1|    2|     1000|    10000|\n",
      "|        2|    1|      500|      500|\n",
      "|        3|10000|        0|        0|\n",
      "+---------+-----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_partition_info(df_test_2.repartition(4, F.col(\"value\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-dispatch",
   "metadata": {},
   "source": [
    "Without using the content of the column \"value\" the repartition method will distribute the messages on a RoundRobin basis. All partitions have almost the same amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "quarterly-syntax",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---------+---------+\n",
      "|partition|count|min_value|max_value|\n",
      "+---------+-----+---------+---------+\n",
      "|        0| 2501|        0|      500|\n",
      "|        1| 2501|        0|    10000|\n",
      "|        2| 2500|        0|        0|\n",
      "|        3| 2501|        0|     1000|\n",
      "+---------+-----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_partition_info(df_test_2.repartition(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-marketplace",
   "metadata": {},
   "source": [
    "This case shows that the Dataframe df2 is not well defined for a repartitioning by ranges as almost all values are 0. Therefore, we even end up having only two partitions whereas the partition 0 contains all Zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "extreme-flower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---------+---------+\n",
      "|partition|count|min_value|max_value|\n",
      "+---------+-----+---------+---------+\n",
      "|        0|10000|        0|        0|\n",
      "|        1|    3|      500|    10000|\n",
      "+---------+-----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_partition_info(df_test_2.repartitionByRange(4, F.col(\"value\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-timer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
