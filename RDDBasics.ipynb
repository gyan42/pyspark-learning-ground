{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mobile-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "chinese-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_CORES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "classical-border",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-192-168-0-142.us-west-2.compute.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://IMCHLT276:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataSkewness</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0558e30c10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://IMCHLT276:7077\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.cores.max\", f\"{MAX_NUM_CORES}\") \\\n",
    "    .config(\"spark.local.dir\", \"/opt/tmp/spark-temp/\") \\\n",
    "    .appName(\"DataSkewness\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "modern-cream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1348\n",
      "  4 drwxrwxr-x  8 mageswarand mageswarand   4096 May  5 22:52 .\n",
      "  4 drwxr-xr-x 29 mageswarand openvpn       4096 May  1 10:51 ..\n",
      "  4 drwxrwxr-x  8 mageswarand mageswarand   4096 Mar 18 12:19 .git\n",
      "  4 drwxrwxr-x  2 mageswarand mageswarand   4096 May  5 22:37 .ipynb_checkpoints\n",
      " 24 -rw-rw-r--  1 mageswarand mageswarand  24187 Mar  1 00:03 Classification.ipynb\n",
      "  4 -rw-rw-r--  1 mageswarand mageswarand   1840 May  5 22:52 Joins.ipynb\n",
      "  4 -rw-rw-r--  1 mageswarand mageswarand    816 Feb 18 18:20 PandasUDF.ipynb\n",
      " 88 -rw-rw-r--  1 mageswarand mageswarand  88508 May  5 22:12 Partition.ipynb\n",
      "680 -rw-rw-r--  1 mageswarand mageswarand 694537 Apr  1 19:33 PySpark_SQL_Cheat_Sheet_Python.pdf\n",
      " 12 -rw-rw-r--  1 mageswarand mageswarand  11084 May  5 22:51 RDDBasics.ipynb\n",
      "  4 -rw-rw-r--  1 mageswarand mageswarand    916 Feb  3 16:10 README.md\n",
      " 16 -rw-rw-r--  1 mageswarand mageswarand  14870 Apr  1 19:46 UDF.ipynb\n",
      " 16 -rw-rw-r--  1 mageswarand mageswarand  15940 Feb  3 13:41 WindowFunction.ipynb\n",
      "  4 drwxrwxr-x  4 mageswarand mageswarand   4096 May  5 19:26 data\n",
      "  4 drwxrwxr-x  8 mageswarand mageswarand   4096 Mar 19 14:30 dataskew-experiments\n",
      "  4 drwxrwxr-x  2 mageswarand mageswarand   4096 Feb 27 15:09 nlp\n",
      " 20 -rw-rw-r--  1 mageswarand mageswarand  19233 May  5 22:01 pyspark_window_function_examples.ipynb\n",
      "  4 drwxrwxr-x  5 mageswarand mageswarand   4096 Feb  3 17:36 test_driven_developement\n",
      "448 -rw-rw-r--  1 mageswarand mageswarand 455154 Feb 28 23:59 training.txt\n",
      "  0 -rw-rw-r--  1 mageswarand mageswarand      0 Feb 18 18:15 untitled.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -als\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "oriented-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/out/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "persistent-detective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1\\tThe Da Vinci Code book is just awesome.', \"1\\tthis was the first clive cussler i've ever read, but even books like Relic, and Da Vinci code were more plausible than this.\", '1\\ti liked the Da Vinci Code a lot.', '1\\ti liked the Da Vinci Code a lot.', \"1\\tI liked the Da Vinci Code but it ultimatly didn't seem to hold it's own.\", \"1\\tthat's not even an exaggeration ) and at midnight we went to Wal-Mart to buy the Da Vinci Code, which is amazing of course.\", '1\\tI loved the Da Vinci Code, but now I want something better and different!..', '1\\ti thought da vinci code was great, same with kite runner.', '1\\tThe Da Vinci Code is actually a good movie...', '1\\tI thought the Da Vinci Code was a pretty good book.', '1\\tThe Da Vinci Code is one of the most beautiful movies ive ever seen.', '1\\tThe Da Vinci Code is an * amazing * book, do not get me wrong.', '1\\tthen I turn on the light and the radio and enjoy my Da Vinci Code.', '1\\tThe Da Vinci Code was REALLY good.', '1\\ti love da vinci code....', '1\\ti loved da vinci code..', '1\\tTO NIGHT:: THE DA VINCI CODE AND A BEAUTIFUL MIND...', '1\\tTHE DA VINCI CODE is AN AWESOME BOOK....', '1\\tThing is, I enjoyed The Da Vinci Code.', '1\\tvery da vinci code slash amazing race.']\n",
      "[('Mountain', 805), ('was', 1164), ('awesome.', 122), ('an', 224), ('1\\tman', 80), ('i', 506), ('loved', 164), ('brokeback', 481), ('1\\tdudeee', 80), ('Brokeback', 901), ('think', 200), ('is', 1346), ('becoming', 81), ('more', 99), ('acceptable!:', 80), ('1\\tAnyway,', 82), ('why', 169), ('love', 1253), ('\"', 411), ('Mountain.', 268)]\n"
     ]
    }
   ],
   "source": [
    "# Create RDD from text file\n",
    "rdd = sc.textFile('training.txt') # file:///\n",
    "\n",
    "# To View the data\n",
    "print(rdd.take(20))\n",
    "\n",
    "# Split lines into words\n",
    "lines_rdd = rdd.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Create Pair RDD by giving 1 to each word\n",
    "# transformed_rdd = transformed_rdd.map(lambda x: x.strip('[').strip(']'))\n",
    "words_rdd = lines_rdd.map(lambda word: (word, 1))\n",
    "\n",
    "# GroupByWord\n",
    "word_count_rdd = words_rdd.reduceByKey(lambda a, b: a+b)\n",
    "\n",
    "# To View the data\n",
    "print(word_count_rdd.take(20))\n",
    "\n",
    "# Write data to out path\n",
    "word_count_rdd.saveAsTextFile('data/out/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "editorial-appreciation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', 'The', 'Da', 'Vinci', 'Code', 'book', 'is', 'just', 'awesome.', '1']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(lambda line: line.split()).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "educational-harvey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,3,5,67,8,4,43,4,43,4,3,3,32,78,7,9,2,4,22,6,2]\n",
    "rdd1 = sc.parallelize(data)\n",
    "rdd1.collect()\n",
    "rdd1.getNumPartitions() # equals to number of cores in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "human-exception",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('was', 1164),\n",
       " ('is', 1346),\n",
       " ('love', 1253),\n",
       " ('Da', 1122),\n",
       " ('Vinci', 1474),\n",
       " ('Code', 1201),\n",
       " ('the', 2080),\n",
       " ('and', 2004),\n",
       " ('a', 1209),\n",
       " ('1\\tI', 1273),\n",
       " ('I', 1983),\n",
       " ('Harry', 1876)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [(word, count)...]\n",
    "word_count_rdd.filter(lambda wc: wc[1]>1000).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "everyday-mobility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[20] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[19] at mapPartitions at PythonRDD.scala:133 []\n",
      " |  ShuffledRDD[18] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(2) PairwiseRDD[17] at sortBy at <ipython-input-12-abbb684fb72a>:1 []\n",
      "    |  PythonRDD[16] at sortBy at <ipython-input-12-abbb684fb72a>:1 []\n",
      "    |  MapPartitionsRDD[6] at mapPartitions at PythonRDD.scala:133 []\n",
      "    |  ShuffledRDD[5] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      "    +-(2) PairwiseRDD[4] at reduceByKey at <ipython-input-8-754b9e2c7702>:15 []\n",
      "       |  PythonRDD[3] at reduceByKey at <ipython-input-8-754b9e2c7702>:15 []\n",
      "       |  training.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "       |  training.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "print(word_count_rdd.sortBy(lambda wc: wc[1]).toDebugString().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-belarus",
   "metadata": {},
   "source": [
    "----------------------\n",
    "For example, when we perform reduceByKey() operation, PySpark does the following\n",
    "- PySpark first runs map tasks on all partitions which groups all values for a single key.\n",
    "- The results of the map tasks are kept in memory.\n",
    "- When results do not fit in memory, PySpark stores the data into a disk.\n",
    "- PySpark shuffles the mapped data across partitions, some times it also stores the shuffled data into a disk for reuse when it needs to recalculate.\n",
    "- Run the garbage collection\n",
    "- Finally runs reduce tasks on each partition based on key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-sussex",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-chart",
   "metadata": {},
   "source": [
    "Broadcast and accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "applied-complaint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar = sc.broadcast([0, 1, 2, 3])\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "criminal-insider",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=1, value=6>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "sc.parallelize([1, 2, 3]).foreach(lambda x: accum.add(x))\n",
    "accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-negative",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-negative",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-drain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-industry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-avenue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-extension",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-replication",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
